â€‹![image](assets/image-20240418235907-30qz7xc.png)â€‹

â€

# å¾®è°ƒ

å¤§æ¨¡å‹ä¸ºä»€ä¹ˆè¦å¾®è°ƒï¼Ÿåœ¨ç»è¿‡å¤§è§„æ¨¡çš„é¢„è®­ç»ƒåï¼Œå…·å¤‡äº†ä¸€èˆ¬æ€§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ä½†æ˜¯ï¼Œå½“æˆ‘ä»¬å°†è¿™æ ·çš„æ¨¡å‹åº”ç”¨åˆ°å…·ä½“åœºæ™¯æ—¶ï¼Œæ¯”å¦‚åŒ»ç–—è¯Šæ–­ã€æ³•å¾‹å’¨è¯¢ã€è´¢ç»åˆ†ææˆ–è€…ç‰¹å®šä¼ä¸šçš„å®¢æœç³»ç»Ÿä¸­ï¼Œç›´æ¥ä½¿ç”¨æœªç»è°ƒæ•´çš„æ¨¡å‹å¯èƒ½æ— æ³•å¾ˆå¥½åœ°å¤„ç†è¿™äº›é¢†åŸŸçš„ä¸“ä¸šé—®é¢˜ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œä¸€ä¸ªæ¨¡å‹å°±åƒä¸€ä½å­¦è¯†æ¸Šåšçš„é€šæ‰ï¼Œä»–è¯»è¿‡å¾ˆå¤šä¹¦ï¼Œèƒ½è§£ç­”å„ç§ä¸€èˆ¬æ€§é—®é¢˜ï¼Œä½†å¦‚æœæˆ‘ä»¬è¦è®©ä»–å»è§£å†³ä¸“ä¸šçš„åŒ»å­¦éš¾é¢˜æˆ–è€…æ’°å†™ä¸“ä¸šçš„æ³•å¾‹æ„è§ä¹¦ï¼Œå°±éœ€è¦å¯¹ä»–è¿›è¡Œé¢å¤–çš„åŸ¹è®­ï¼Œè®©ä»–ç†Ÿæ‚‰åŒ»å­¦æœ¯è¯­ã€ç—…ä¾‹åˆ†ææˆ–è€…æ³•å¾‹æ¡æ¬¾ç­‰å†…å®¹ã€‚

å› æ­¤ï¼Œåœ¨å…·ä½“åœºæ™¯ä¸‹å¯¹å¤§æ¨¡å‹è¿›è¡Œå¾®è°ƒå°±åƒæ˜¯ç»™è¿™ä½é€šæ‰æä¾›ä¸“ä¸šè¿›ä¿®è¯¾ç¨‹ï¼Œè®©å®ƒé’ˆå¯¹ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡è¿›è¡Œæ·±å…¥å­¦ä¹ ã€‚é€šè¿‡ä½¿ç”¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†è¿›è¡Œå†è®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¹¶å¤„ç†è¿™ä¸ªé¢†åŸŸçš„ç‰¹æœ‰é—®é¢˜ï¼Œè¿›è€Œæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ç²¾å‡†åº¦å’Œæ•ˆæœã€‚è¿™æ ·ä¸€æ¥ï¼Œå¾®è°ƒåçš„æ¨¡å‹å°±èƒ½æ›´ç¬¦åˆç‰¹å®šåœºæ™¯çš„éœ€æ±‚ï¼Œå‘æŒ¥å‡ºæ›´å¤§çš„ä»·å€¼ã€‚

â€

> åŸæœ¬æŸä¸ªé¢†åŸŸèƒ½è¾¾åˆ°80åˆ†ï¼Œè€Œå¾®è°ƒå°±æ˜¯è¦è¾¾åˆ°95åˆ†ã€‚

â€

## ä¸¤ç§å¾®è°ƒèŒƒå¼

LLMçš„ä¸‹æ¸¸åº”ç”¨ä¸­ï¼Œå¢é‡é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è·Ÿéšæ˜¯ç»å¸¸ä¼šç”¨åˆ°ä¸¤ç§çš„å¾®è°ƒæ¨¡å¼

â€

1. å¢é‡é¢„è®­ç»ƒå¾®è°ƒ

    ä½¿ç”¨åœºæ™¯ï¼šè®©åŸºåº§æ¨¡å‹å­¦ä¹ åˆ°ä¸€äº›æ–°çŸ¥è¯†ï¼Œå¦‚æŸä¸ªå‚ç±»é¢†åŸŸçš„å¸¸è¯†  
    **è®­ç»ƒæ•°æ®ï¼šæ–‡ç« ã€ä¹¦ç±ã€ä»£ç ç­‰**

2. æŒ‡ä»¤è·Ÿéšå¾®è°ƒ

    ä½¿ç”¨åœºæ™¯ï¼šè®©æ¨¡å‹å­¦ä¼šå¯¹è¯æ¨¡æ¿ï¼Œæ ¹æ®äººç±»æŒ‡ä»¤è¿›è¡Œå¯¹è¯  
    **è®­ç»ƒæ•°æ®ï¼šé«˜è´¨é‡çš„å¯¹è¯ã€é—®ç­”æ•°æ®**

â€

â€‹![image](assets/image-20240419001141-eb1zsxt.png)â€‹

â€‹â€‹

å…·ä½“åœºæ™¯ä½“ç°ï¼š

åœ¨æ²¡æœ‰é€šè¿‡æŒ‡ä»¤å¾®è°ƒä¹‹å‰ï¼Œæ¨¡å‹å¹¶ä¸ç†è§£è¿™æ˜¯ä¸€ä¸ªå¯¹è¯ï¼Œè€Œæ˜¯è®¤ä¸ºæ˜¯ä¸€ä¸ªæ‹Ÿåˆï¼Œè¾“å‡ºæ›´å¤šç›¸ä¼¼çš„å†…å®¹ã€‚æ‰€ä»¥Chatæ¨¡å‹å°±æ˜¯é€šè¿‡æŒ‡ä»¤å¾®è°ƒå¾—æ¥çš„ã€‚

â€‹![image](assets/image-20240419001318-eyrf22y.png)â€‹

â€

### å¾®è°ƒæ•°æ®çš„æµç¨‹

ä»æ–‡ç« ä¸­çš„ä¸€æ®µå†…å®¹ï¼Œæˆ–è€…ä¸€ä¸ªé™ˆè¿°å¥ï¼Œæˆ–æ˜¯ä¸€ä»£ç éƒ½æ˜¯åŸå§‹æ•°æ®ã€‚é€šè¿‡å¤šä¸ªæ­¥éª¤åæœ€ç»ˆæˆä¸ºè®­ç»ƒæ•°æ®ã€‚

â€‹![image](assets/image-20240419001640-kevapzz.png)â€‹

â€‹![image](assets/image-20240419001727-izgufwb.png)â€‹

â€

â€

### å¯¹è¯æ¨¡æ¿

å¯¹è¯æ¨¡æ¿æ˜¯ä¸ºäº†èƒ½å¤Ÿè®©LLMåŒºåˆ†å‡ºï¼ŒSystemã€Userå’ŒAssistant,ä¸åŒçš„æ¨¡å‹ä¼šæœ‰ä¸åŒçš„æ¨¡æ¿ã€‚

â€‹![image](assets/image-20240419001833-a8mrpc4.png)â€‹

â€‹![image](assets/image-20240419001844-5ajevui.png)â€‹

â€

ä¸ºäº†è®©LLMåŒºåˆ†å‡ºï¼Œå¼€å§‹ä¸ç»“æŸï¼Œé—®é¢˜ä¸å›ç­”ã€‚éœ€è¦è¿›è¡Œä¸€äº›é¢å¤–çš„æ ‡æ³¨ã€‚

â€‹![image](assets/image-20240419002206-y7bm2qj.png)â€‹

â€‹![image](assets/image-20240419002210-i7nlgi1.png)â€‹

â€

â€

## LoRAå’ŒQLoRA

LLMçš„å‚æ•°é‡ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹ä¸­çš„Linear,è®­ç»ƒè¿™äº›å‚æ•°ä¼šè€—è´¹å¤§é‡çš„æ˜¾å­˜

LoRAé€šè¿‡åœ¨åŸæœ¬çš„Linearæ—ï¼Œæ–°å¢ä¸€ä¸ªæ”¯è·¯ï¼ŒåŒ…å«ä¸¤ä¸ªè¿ç»­çš„å°Linear,æ–°å¢çš„è¿™ä¸ªæ”¯è·¯é€šå¸¸å«åšAdapter

Adapterå‚æ•°é‡è¿œå°äºåŸæœ¬çš„Linear,èƒ½å¤§å¹…é™ä½è®­å·ç»ƒçš„æ˜¾å­˜æ¶ˆè€—

â€‹![image](assets/image-20240419002316-9b4a4ff.png)â€‹

â€

ç®€å•çš„ç†è§£ï¼š

â€‹![image](assets/image-20240419002346-o7swumd.png)â€‹

â€

### ä¸‰ç§è®­ç»ƒæ–¹æ¡ˆå¯¹æ¯”

å¯¹æ¯”å…¨å‚æ•°å¾®è°ƒï¼ŒLoRAå’ŒQLoRAä¸‰ç§æ–¹å¼å¯¹æ¯”ã€‚

â€

â€‹![image](assets/image-20240419002434-vqbr6vm.png)â€‹

å…¨å‚æ•°ï¼šBase Modelå‚ä¸è®­ç»ƒå¹¶æ›´æ–°å‚æ•°ã€‚éœ€è¦ä¿å­˜Base Modelä¸­å‚æ•°çš„ä¼˜åŒ–å™¨çŠ¶æ€

LoRAï¼šBase Modelåªå‚ä¸Forwardã€‚åªæœ‰Adapteréƒ¨åˆ†Backwardæ›´æ–°å‚æ•°ã€‚åªéœ€ä¿å­˜Adapterä¸­å‚æ•°çš„ä¼˜åŒ–å™¨çŠ¶æ€

QLoRAï¼šBase Modelé‡åŒ–ä¸º4-bitã€‚ä¼˜åŒ–å™¨çŠ¶æ€åœ¨CPUä¸GPUé—´Offloadã€‚Base Modelåªå‚ä¸Forwardã€‚åªæœ‰Adapteréƒ¨åˆ†Backwardæ›´æ–°å‚æ•°ã€‚åªéœ€ä¿å­˜Adapterä¸­å‚æ•°çš„ä¼˜åŒ–å™¨çŠ¶æ€

â€

â€

# å¾®è°ƒå·¥å…·XTuner

XTuner ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹&å¤šæ¨¡æ€æ¨¡å‹å¾®è°ƒå·¥å…·ç®±ã€‚*ç”±* *MMRazor* *å’Œ* *MMDeploy* *è”åˆå¼€å‘ã€‚*

* ğŸ¤“ **å‚»ç“œåŒ–ï¼š**  ä»¥ é…ç½®æ–‡ä»¶ çš„å½¢å¼å°è£…äº†å¤§éƒ¨åˆ†å¾®è°ƒåœºæ™¯ï¼Œ**0åŸºç¡€çš„éä¸“ä¸šäººå‘˜ä¹Ÿèƒ½ä¸€é”®å¼€å§‹å¾®è°ƒ**ã€‚
* ğŸƒ **è½»é‡çº§ï¼š**  å¯¹äº 7B å‚æ•°é‡çš„LLMï¼Œ**å¾®è°ƒæ‰€éœ€çš„æœ€å°æ˜¾å­˜ä»…ä¸º 8GB** ï¼š **æ¶ˆè´¹çº§æ˜¾å¡âœ…ï¼Œcolabâœ…**

æŠ€æœ¯æ¶æ„å›¾ï¼š

â€‹![image](assets/image-20240419002658-pvr67kj.png)â€‹

* å¤šç§å¾®è°ƒç®—æ³•ï¼šå¤šç§å¾®è°ƒç­–ç•¥ä¸ç®—æ³•ï¼Œè¦†ç›–å„ç±»SFTåœºæ™¯é€‚é…

  å¤šç§å¼€æºç”Ÿæ€ï¼šæ”¯æŒåŠ è½½HuggingFaceã€ModelScopeæ¨¡å‹æˆ–æ•°æ®é›†

  è‡ªåŠ¨ä¼˜åŒ–åŠ é€Ÿï¼šå¼€å‘è€…æ— éœ€å…³æ³¨å¤æ‚çš„æ˜¾å­˜ä¼˜åŒ–ä¸è®¡ç®—åŠ é€Ÿç»†èŠ‚

* é€‚é…å¤šç§ç¡¬ä»¶ï¼š

  è®­ç»ƒæ–¹æ¡ˆè¦†ç›–NVIDIA20ç³»ä»¥ä¸Šæ‰€æœ‰æ˜¾å¡  

  æœ€ä½åªéœ€8GBæ˜¾å­˜å³å¯å¾®è°ƒ7Bæ¨¡å‹

â€

XTuner çš„è¿è¡ŒåŸç†ã€‚

â€‹![image](assets/image-20240419003707-wy1ltwf.png)â€‹

â€

æ“ä½œæ€è·¯ï¼š

1. **ç¯å¢ƒå®‰è£…**ï¼šç¬¬ä¸€æ­¥å¿…ç„¶å°±æ˜¯å®‰è£…XTunerï¼
2. **å‰æœŸå‡†å¤‡**ï¼šæ˜ç¡®æˆ‘ä»¬è‡ªå·±çš„å¾®è°ƒç›®æ ‡äº†ã€‚æˆ‘ä»¬æƒ³è¦åˆ©ç”¨å¾®è°ƒåšä¸€äº›ä»€ä¹ˆäº‹æƒ…å‘¢ï¼Œé‚£æˆ‘ä¸ºäº†åšåˆ°è¿™ä¸ªäº‹æƒ…æˆ‘æœ‰å“ªäº›ç¡¬ä»¶çš„èµ„æºå’Œæ•°æ®å‘¢ï¼Ÿå‡å¦‚æˆ‘ä»¬æœ‰å¯¹äºä¸€ä»¶äº‹æƒ…ç›¸å…³çš„æ•°æ®é›†ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜æœ‰è¶³å¤Ÿçš„ç®—åŠ›èµ„æºï¼Œé‚£å½“ç„¶å¾®è°ƒå°±æ˜¯ä¸€ä»¶æ°´åˆ°æ¸ æˆçš„äº‹æƒ…ã€‚ä½†æ˜¯å¯¹äºæ™®é€šçš„å¼€å‘è€…è€Œè¨€ï¼Œåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½å°±éœ€è¦è€ƒè™‘æ€ä¹ˆé‡‡é›†æ•°æ®ï¼Œç”¨ä»€ä¹ˆæ ·çš„æ‰‹æ®µå’Œæ–¹å¼æ¥è®©æ¨¡å‹æœ‰æ›´å¥½çš„æ•ˆæœã€‚
3. **å¯åŠ¨å¾®è°ƒ**ï¼šåœ¨ç¡®å®šäº†è‡ªå·±çš„å¾®è°ƒç›®æ ‡åï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨ XTuner çš„é…ç½®åº“ä¸­æ‰¾åˆ°åˆé€‚çš„é…ç½®æ–‡ä»¶å¹¶è¿›è¡Œå¯¹åº”çš„ä¿®æ”¹ã€‚ä¿®æ”¹å®Œæˆåå³å¯ä¸€é”®å¯åŠ¨è®­ç»ƒï¼è®­ç»ƒå¥½çš„æ¨¡å‹ä¹Ÿå¯ä»¥ä»…ä»…é€šè¿‡åœ¨ç»ˆç«¯è¾“å…¥ä¸€è¡ŒæŒ‡ä»¤æ¥å®Œæˆè½¬æ¢å’Œéƒ¨ç½²å·¥ä½œï¼

â€

â€

## å¿«é€Ÿä¸Šæ‰‹

å¼€å‘æœºé€‰æ‹© 10*A100 å³å¯

CUDAï¼š11.7

â€

### å®‰è£…ä¸é…ç½®

1. **å®‰è£…**

```python
pip install xtuner
```

2. **æŒ‘é€‰é…ç½®æ¨¡æ¿**

```python
xtuner list-cfg -p internlm_1_8b
```

3. **ä¸€é”®è®­ç»ƒ**

```python
xtuner train internlm_1_8b_glora_oasst1_512_e3
```

Configå‘½åè§„åˆ™

â€‹![image](assets/image-20240419003205-moy8xh9.png)â€‹

â€

4. æ‹·è´é…ç½®æ¨¡æ¿  

    ```python
    xtuner copy-cfg internlm_20b_glora_oasst1_512_e3 ./
    ```
5. ä¿®æ”¹é…ç½®æ¨¡æ¿  

    ```python
    vi internlm_20b_glora_oasst1_512_e3_copy.py
    ```
6. å¯åŠ¨è®­ç»ƒ

    ```python
    xtuner train internlm_20b_glora_oasst1_512_e3_copy.py
    ```

å¸¸ç”¨è¶…å‚

â€‹![image](assets/image-20240419004242-iomy9jm.png)â€‹

â€

å¸¸ç”¨çš„åŸºæœ¬é…ç½®

â€‹![image](assets/image-20240419004259-zxkh1vs.png)â€‹

â€

### å¯¹è¯

7. å¯¹è¯ï¼šä¸ºäº†ä¾¿äºå¼€å‘è€…æŸ¥çœ‹è®­å·ç»ƒæ•ˆæœï¼ŒXtuneræä¾›äº†ä¸€é”®å¯¹è¯æ¥å£ã€‚![image](assets/image-20240419004531-ddoidso.png)â€‹

    Float16æ¨¡å‹å¯¹è¯

    â€

    ```python
    xtuner chat internlm/internlm-chat-20b
    ```

    4bitæ¨¡å‹å¯¹è¯  

    ```python
    xtuner chat internlm/internlm-chat-20b --bits 4
    ```

    åŠ è½½Adapteræ¨¡å‹å¯¹è¯

    ```python
    xtuner chat internlm/internlm-chat-20b --adapater $ADAPTER DIR
    ```

â€

XTunerè¿˜æ”¯æŒå·¥å…·ç±»æ¨¡å‹çš„å¯¹è¯ï¼Œ

â€‹![image](assets/image-20240419004643-e9uo347.png)â€‹

â€

XTuneræ•°æ®å¼•æ“

å°†åŸºæœ¬çš„å¯¹è¯å½¢å¼è‡ªåŠ¨è½¬åŒ–ä¸ºï¼Œæ¨¡å‹æ‰€æ”¯æŒçš„å¯¹è¯æ ¼å¼ã€‚

â€‹![image](assets/image-20240419004712-qmara5l.png)â€‹

â€

## åŠ é€Ÿæ¨ç†åŸç†

Flash Attentionå’ŒDeepSpeed ZeROæ˜¯XTuneræœ€é‡è¦çš„ä¸¤ä¸ªä¼˜åŒ–æŠ€å·§

* Flash Attentionï¼ˆè‡ªåŠ¨å¼€å¯ï¼‰ï¼šFlash Attentionå°†Attentionè®¡ç®—å¹¶è¡ŒåŒ–ï¼Œé¿å…äº†è®¡ç®—è¿‡ç¨‹ä¸­Attention Score NxNçš„æ˜¾å­˜å ç”¨ï¼ˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„Néƒ½æ¯”è¾ƒå¤§ï¼‰

* **DeepSpeed ZeRO**ï¼ˆæ‰‹åŠ¨å¼€å¯ï¼‰ï¼šZeROä¼˜åŒ–ï¼Œé€šè¿‡å°†è®­å·ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€åˆ‡ç‰‡ä¿å­˜ï¼Œèƒ½å¤Ÿåœ¨å¤šGPUè®­ç»ƒæ—¶æ˜¾è‘—èŠ‚çœæ˜¾å­˜ã€‚

  é™¤äº†å°†è®­ç»ƒä¸­é—´çŠ¶æ€åˆ‡ç‰‡å¤–DeepSpeedè®­ç»ƒæ—¶ä½¿ç”¨FP16çš„æƒé‡ï¼Œç›¸è¾ƒäºPytorchçš„AMPè®­ç»ƒï¼Œåœ¨å•GPUä¸Šä¹Ÿèƒ½å¤§å¹…èŠ‚çœæ˜¾å­˜

é€šè¿‡XTunerå³å¯å¿«é€Ÿç®€å•çš„å¼€å¯ä¼˜åŒ–ã€‚

â€‹![image](assets/image-20240419005020-3bhd23p.png)â€‹

â€

â€

## å¾®è°ƒå®æˆ˜

åŸºäºInternLM2 1.8Bæ¥è¿›è¡Œå¾®è°ƒ

â€‹![image](assets/image-20240419005216-d2vim8j.png)â€‹

â€

### 2.1 ç¯å¢ƒå®‰è£…

é¦–å…ˆæˆ‘ä»¬éœ€è¦å…ˆå®‰è£…ä¸€ä¸ª XTuner çš„æºç åˆ°æœ¬åœ°æ¥æ–¹ä¾¿åç»­çš„ä½¿ç”¨ã€‚

```shell
# å¦‚æœä½ æ˜¯åœ¨ InternStudio å¹³å°ï¼Œåˆ™ä»æœ¬åœ° clone ä¸€ä¸ªå·²æœ‰ pytorch çš„ç¯å¢ƒï¼š
# pytorch    2.0.1   py3.10_cuda11.7_cudnn8.5.0_0

studio-conda xtuner0.1.17
# å¦‚æœä½ æ˜¯åœ¨å…¶ä»–å¹³å°ï¼š
# conda create --name xtuner0.1.17 python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate xtuner0.1.17
# è¿›å…¥å®¶ç›®å½• ï¼ˆ~çš„æ„æ€æ˜¯ â€œå½“å‰ç”¨æˆ·çš„homeè·¯å¾„â€ï¼‰
cd ~
# åˆ›å»ºç‰ˆæœ¬æ–‡ä»¶å¤¹å¹¶è¿›å…¥ï¼Œä»¥è·Ÿéšæœ¬æ•™ç¨‹
mkdir -p /root/xtuner0117 && cd /root/xtuner0117

# æ‹‰å– 0.1.17 çš„ç‰ˆæœ¬æºç 
git clone -b v0.1.17  https://github.com/InternLM/xtuner
# æ— æ³•è®¿é—®githubçš„ç”¨æˆ·è¯·ä» gitee æ‹‰å–:
# git clone -b v0.1.15 https://gitee.com/Internlm/xtuner

# è¿›å…¥æºç ç›®å½•
cd /root/xtuner0117/xtuner

# ä»æºç å®‰è£… XTuner
pip install -e '.[all]'
```

> å‡å¦‚é€Ÿåº¦å¤ªæ…¢å¯ä»¥ `Ctrl + C`â€‹â€‹ é€€å‡ºåæ¢æˆ `pip install -e '.[all]' -i https://mirrors.aliyun.com/pypi/simple/`â€‹â€‹

â€

### 2.2 å‰æœŸå‡†å¤‡

#### 2.2.1 æ•°æ®é›†å‡†å¤‡

> æ²¡æœ‰å®é™…çš„å¯ç”¨æ•°æ®é›†ï¼Œæ‰€ä»¥å°±ç®€å•çš„åŸºäºä¸€ä¸ªè®¤çŸ¥ç¤ºä¾‹æ¥å®Œæˆå¾®è°ƒä»»åŠ¡ã€‚

ä¸ºäº†è®©æ¨¡å‹èƒ½å¤Ÿè®©æ¨¡å‹è®¤æ¸…è‡ªå·±çš„èº«ä»½å¼Ÿä½ï¼ŒçŸ¥é“åœ¨è¯¢é—®è‡ªå·±æ˜¯è°çš„æ—¶å€™å›å¤æˆæˆ‘ä»¬æƒ³è¦çš„æ ·å­ï¼Œæˆ‘ä»¬å°±éœ€è¦é€šè¿‡åœ¨å¾®è°ƒæ•°æ®é›†ä¸­å¤§é‡æºæ‚è¿™éƒ¨åˆ†çš„æ•°æ®ã€‚

é¦–å…ˆæˆ‘ä»¬å…ˆåˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹æ¥å­˜æ”¾æˆ‘ä»¬è¿™æ¬¡è®­ç»ƒæ‰€éœ€è¦çš„æ‰€æœ‰æ–‡ä»¶ã€‚

```shell
# å‰åŠéƒ¨åˆ†æ˜¯åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼ŒååŠéƒ¨åˆ†æ˜¯è¿›å…¥è¯¥æ–‡ä»¶å¤¹ã€‚
mkdir -p /root/ft && cd /root/ft

# åœ¨ftè¿™ä¸ªæ–‡ä»¶å¤¹é‡Œå†åˆ›å»ºä¸€ä¸ªå­˜æ”¾æ•°æ®çš„dataæ–‡ä»¶å¤¹
mkdir -p /root/ft/data && cd /root/ft/data
```

ä¹‹åæˆ‘ä»¬å¯ä»¥åœ¨ `data`â€‹ ç›®å½•ä¸‹æ–°å»ºä¸€ä¸ª `generate_data.py`â€‹ æ–‡ä»¶ï¼Œå°†ä»¥ä¸‹ä»£ç å¤åˆ¶è¿›å»ï¼Œç„¶åè¿è¡Œè¯¥è„šæœ¬å³å¯ç”Ÿæˆæ•°æ®é›†ã€‚å‡å¦‚æƒ³è¦åŠ å¤§å‰‚é‡è®©ä»–èƒ½å¤Ÿå®Œå®Œå…¨å…¨è®¤è¯†åˆ°ä½ çš„èº«ä»½ï¼Œé‚£æˆ‘ä»¬å¯ä»¥å§ `n`â€‹ çš„å€¼è°ƒå¤§ä¸€ç‚¹ã€‚

```shell
# åˆ›å»º `generate_data.py` æ–‡ä»¶
touch /root/ft/data/generate_data.py
```

å‘½ä»¤è¡Œçš„æ–¹å¼ç¼–è¾‘æ–‡ä»¶ï¼ˆå»ºè®®è¿˜æ˜¯å­¦è¿‡çš„äººå†ä½¿ç”¨å§ï¼‰ï¼Œç²˜è´´ä¸‹é¢çš„å†…å®¹å `ESC`â€‹  `:wq`â€‹

```python
vim generate_data.py
```

æ‰“å¼€è¯¥ python æ–‡ä»¶åå°†ä¸‹é¢çš„å†…å®¹å¤åˆ¶è¿›å»ã€‚

```python
import json

# è®¾ç½®ç”¨æˆ·çš„åå­—
name = 'ä¸è¦å§œè‘±è’œå¤§ä½¬'
# è®¾ç½®éœ€è¦é‡å¤æ·»åŠ çš„æ•°æ®æ¬¡æ•°
n =  10000

# åˆå§‹åŒ–OpenAIæ ¼å¼çš„æ•°æ®ç»“æ„
data = [
    {
        "messages": [
            {
                "role": "user",
                "content": "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»"
            },
            {
                "role": "assistant",
                "content": "æˆ‘æ˜¯{}çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦".format(name)
            }
        ]
    }
]

# é€šè¿‡å¾ªç¯ï¼Œå°†åˆå§‹åŒ–çš„å¯¹è¯æ•°æ®é‡å¤æ·»åŠ åˆ°dataåˆ—è¡¨ä¸­
for i in range(n):
    data.append(data[0])

# å°†dataåˆ—è¡¨ä¸­çš„æ•°æ®å†™å…¥åˆ°ä¸€ä¸ªåä¸º'personal_assistant.json'çš„æ–‡ä»¶ä¸­
with open('personal_assistant.json', 'w', encoding='utf-8') as f:
    # ä½¿ç”¨json.dumpæ–¹æ³•å°†æ•°æ®ä»¥JSONæ ¼å¼å†™å…¥æ–‡ä»¶
    # ensure_ascii=False ç¡®ä¿ä¸­æ–‡å­—ç¬¦æ­£å¸¸æ˜¾ç¤º
    # indent=4 ä½¿å¾—æ–‡ä»¶å†…å®¹æ ¼å¼åŒ–ï¼Œä¾¿äºé˜…è¯»
    json.dump(data, f, ensure_ascii=False, indent=4)
```

å¹¶å°†æ–‡ä»¶ `name`â€‹ åé¢çš„å†…å®¹ä¿®æ”¹ä¸ºä½ çš„åç§°ã€‚æ¯”å¦‚è¯´æˆ‘æ˜¯å‰‘é”‹å¤§ä½¬çš„è¯å°±æ˜¯ï¼š

```diff
# å°†å¯¹åº”çš„nameè¿›è¡Œä¿®æ”¹ï¼ˆåœ¨ç¬¬4è¡Œçš„ä½ç½®ï¼‰
- name = 'ä¸è¦å§œè‘±è’œå¤§ä½¬'
+ name = "å‰‘é”‹å¤§ä½¬"
```

ä¿®æ”¹å®Œæˆåè¿è¡Œ `generate_data.py`â€‹ æ–‡ä»¶å³å¯ã€‚

```shell
# ç¡®ä¿å…ˆè¿›å…¥è¯¥æ–‡ä»¶å¤¹
cd /root/ft/data

# è¿è¡Œä»£ç 
python /root/ft/data/generate_data.py
```

dataçš„è·¯å¾„ä¸‹ä¾¿ç”Ÿæˆäº†ä¸€ä¸ªåä¸º `personal_assistant.json`â€‹ çš„æ–‡ä»¶ï¼Œé‡Œé¢å°±åŒ…å«äº† 5000 æ¡ `input`â€‹ å’Œ `output`â€‹ çš„å¯ç”¨äºå¾®è°ƒçš„æ•°æ®å¯¹ã€‚å‡å¦‚ æˆ‘ä»¬è®¤ä¸º 5000 æ¡ä¸å¤Ÿçš„è¯ä¹Ÿå¯ä»¥è°ƒæ•´æ–‡ä»¶ä¸­ç¬¬6è¡Œ `n`â€‹ çš„å€¼å“¦ï¼

```
|-- data/
    |-- personal_assistant.json
    |-- generate_data.py
```

éå¿…è¦æ­¥éª¤ï¼š

â€‹![image](assets/image-20240419015616-n8kf5ri.png)â€‹

â€

> é™¤äº†æˆ‘ä»¬è‡ªå·±é€šè¿‡è„šæœ¬çš„æ•°æ®é›†ï¼Œå…¶å®ç½‘ä¸Šä¹Ÿæœ‰å¤§é‡çš„å¼€æºæ•°æ®é›†å¯ä»¥ä¾›æˆ‘ä»¬è¿›è¡Œä½¿ç”¨ã€‚æœ‰äº›æ—¶å€™æˆ‘ä»¬å¯ä»¥åœ¨å¼€æºæ•°æ®é›†çš„åŸºç¡€ä¸Šæ·»åŠ ä¸€äº›æˆ‘ä»¬è‡ªå·±ç‹¬æœ‰çš„æ•°æ®é›†ï¼Œä¹Ÿå¯èƒ½ä¼šæœ‰å¾ˆå¥½çš„æ•ˆæœã€‚

â€

#### 2.2.2 æ¨¡å‹å‡†å¤‡

åœ¨å‡†å¤‡å¥½äº†æ•°æ®é›†åï¼Œæ¥ä¸‹æ¥å‡†å¤‡å¾®è°ƒçš„æ¨¡å‹ã€‚

ç”±äºæœ¬æ¬¡è¯¾ç¨‹æ˜¾å­˜æ–¹é¢çš„é™åˆ¶ï¼Œè¿™é‡Œæˆ‘ä»¬å°±ä½¿ç”¨ InternLM æœ€æ–°æ¨å‡ºçš„å°æ¨¡å‹ `InterLM2-Chat-1.8B`â€‹ æ¥å®Œæˆæ­¤æ¬¡çš„å¾®è°ƒæ¼”ç¤ºã€‚

å¯¹äºåœ¨ InternStudio ä¸Šè¿è¡Œçš„å°ä¼™ä¼´ä»¬ï¼Œå¯ä»¥ä¸ç”¨é€šè¿‡ OpenXLab æˆ–è€… Modelscope è¿›è¡Œæ¨¡å‹çš„ä¸‹è½½ã€‚æˆ‘ä»¬ç›´æ¥é€šè¿‡ä»¥ä¸‹ä»£ç ä¸€é”®åˆ›å»ºæ–‡ä»¶å¤¹å¹¶å°†æ‰€æœ‰æ–‡ä»¶å¤åˆ¶è¿›å»ã€‚

```shell
# åˆ›å»ºç›®æ ‡æ–‡ä»¶å¤¹ï¼Œç¡®ä¿å®ƒå­˜åœ¨ã€‚
# -pé€‰é¡¹æ„å‘³ç€å¦‚æœä¸Šçº§ç›®å½•ä¸å­˜åœ¨ä¹Ÿä¼šä¸€å¹¶åˆ›å»ºï¼Œä¸”å¦‚æœç›®æ ‡æ–‡ä»¶å¤¹å·²å­˜åœ¨åˆ™ä¸ä¼šæŠ¥é”™ã€‚
mkdir -p /root/ft/model

# å¤åˆ¶å†…å®¹åˆ°ç›®æ ‡æ–‡ä»¶å¤¹ã€‚-ré€‰é¡¹è¡¨ç¤ºé€’å½’å¤åˆ¶æ•´ä¸ªæ–‡ä»¶å¤¹ã€‚
cp -r /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b/* /root/ft/model/
```

é€šè¿‡åˆšæ‰çš„éå†ä»£ç æŸ¥çœ‹ï¼Œå¯ä»¥çœ‹åˆ°åœ¨ model æ–‡ä»¶å¤¹ä¸‹ä¿å­˜äº†æ¨¡å‹çš„ç›¸å…³æ–‡ä»¶å’Œå†…å®¹äº†ã€‚

```
|-- model/
    |-- tokenizer.model
    |-- config.json
    |-- tokenization_internlm2.py
    |-- model-00002-of-00002.safetensors
    |-- tokenizer_config.json
    |-- model-00001-of-00002.safetensors
    |-- model.safetensors.index.json
    |-- configuration.json
    |-- special_tokens_map.json
    |-- modeling_internlm2.py
    |-- README.md
    |-- configuration_internlm2.py
    |-- generation_config.json
    |-- tokenization_internlm2_fast.py
```

å‡å¦‚å¤§å®¶å­˜å‚¨ç©ºé—´ä¸è¶³ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç ä¸€é”®é€šè¿‡ç¬¦å·é“¾æ¥çš„æ–¹å¼é“¾æ¥åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¿™æ ·æ—¢èŠ‚çœäº†ç©ºé—´ï¼Œä¹Ÿä¾¿äºç®¡ç†ã€‚

```shell
# åˆ é™¤/root/ft/modelç›®å½•
rm -rf /root/ft/model

# åˆ›å»ºç¬¦å·é“¾æ¥
ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b /root/ft/model
```

æ‰§è¡Œä¸Šè¿°æ“ä½œåï¼Œ`/root/ft/model`â€‹ å°†ç›´æ¥æˆä¸ºä¸€ä¸ªç¬¦å·é“¾æ¥ï¼Œè¿™ä¸ªé“¾æ¥æŒ‡å‘ `/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b`â€‹ çš„ä½ç½®ã€‚

è¿™æ„å‘³ç€ï¼Œå½“æˆ‘ä»¬è®¿é—® `/root/ft/model`â€‹ æ—¶ï¼Œå®é™…ä¸Šå°±æ˜¯åœ¨è®¿é—® `/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b`â€‹ ç›®å½•ä¸‹çš„å†…å®¹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æ— éœ€å¤åˆ¶ä»»ä½•æ•°æ®ï¼Œå°±å¯ä»¥ç›´æ¥åˆ©ç”¨ç°æœ‰çš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œåç»­çš„å¾®è°ƒæ“ä½œï¼Œä»è€ŒèŠ‚çœå­˜å‚¨ç©ºé—´å¹¶ç®€åŒ–æ–‡ä»¶ç®¡ç†ã€‚

åœ¨è¯¥æƒ…å†µä¸‹çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼Œå¯ä»¥çœ‹åˆ°å’Œä¸Šé¢çš„åŒºåˆ«åœ¨äºå¤šäº†ä¸€äº›è½¯é“¾æ¥ç›¸å…³çš„æ–‡ä»¶ã€‚

```
|-- model/
    |-- tokenizer.model
    |-- config.json
    |-- .mdl
    |-- tokenization_internlm2.py
    |-- model-00002-of-00002.safetensors
    |-- tokenizer_config.json
    |-- model-00001-of-00002.safetensors
    |-- model.safetensors.index.json
    |-- configuration.json
    |-- .msc
    |-- special_tokens_map.json
    |-- .mv
    |-- modeling_internlm2.py
    |-- README.md
    |-- configuration_internlm2.py
    |-- generation_config.json
    |-- tokenization_internlm2_fast.py
```

#### 2.2.3 é…ç½®æ–‡ä»¶é€‰æ‹©

åœ¨å‡†å¤‡å¥½äº†æ¨¡å‹å’Œæ•°æ®é›†åï¼Œæˆ‘ä»¬å°±è¦æ ¹æ®æˆ‘ä»¬é€‰æ‹©çš„å¾®è°ƒæ–¹æ³•æ–¹æ³•ç»“åˆå‰é¢çš„ä¿¡æ¯æ¥æ‰¾åˆ°ä¸æˆ‘ä»¬æœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶äº†ï¼Œä»è€Œå‡å°‘æˆ‘ä»¬å¯¹é…ç½®æ–‡ä»¶çš„ä¿®æ”¹é‡ã€‚

æ‰€è°“é…ç½®æ–‡ä»¶ï¼ˆconfigï¼‰ï¼Œå…¶å®æ˜¯ä¸€ç§ç”¨äºå®šä¹‰å’Œæ§åˆ¶æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­å„ä¸ªæ–¹é¢çš„å‚æ•°å’Œè®¾ç½®çš„å·¥å…·ã€‚å‡†å¤‡å¥½çš„é…ç½®æ–‡ä»¶åªè¦è¿è¡Œèµ·æ¥å°±ä»£è¡¨ç€æ¨¡å‹å°±å¼€å§‹è®­ç»ƒæˆ–è€…å¾®è°ƒäº†ã€‚

XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š

> å¼€ç®±å³ç”¨æ„å‘³ç€å‡å¦‚èƒ½å¤Ÿè¿æ¥ä¸Š Huggingface ä»¥åŠæœ‰è¶³å¤Ÿçš„æ˜¾å­˜ï¼Œå…¶å®å°±å¯ä»¥ç›´æ¥è¿è¡Œè¿™äº›é…ç½®æ–‡ä»¶ï¼ŒXTunerå°±èƒ½å¤Ÿç›´æ¥ä¸‹è½½å¥½è¿™äº›æ¨¡å‹å’Œæ•°æ®é›†ç„¶åå¼€å§‹è¿›è¡Œå¾®è°ƒ

```shell
# åˆ—å‡ºæ‰€æœ‰å†…ç½®é…ç½®æ–‡ä»¶
# xtuner list-cfg

# å‡å¦‚æˆ‘ä»¬æƒ³æ‰¾åˆ° internlm2-1.8b æ¨¡å‹é‡Œæ”¯æŒçš„é…ç½®æ–‡ä»¶
xtuner list-cfg -p internlm2_1_8b
```

åˆ†åˆ«æ˜¯å…¨é‡å¾®è°ƒå’Œqloraå¾®è°ƒçš„ä¸¤ä¸ªæ¨¡å‹ï¼Œalpacaæ•°æ®é›†ï¼Œe3å°±æ˜¯ä¸‰è½®çš„ä»ªå¼ã€‚

â€‹![image](assets/image-20240419020056-7ysgt74.png)â€‹

â€

> è¿™é‡Œå°±ç”¨åˆ°äº†ç¬¬ä¸€ä¸ª XTuner çš„å·¥å…· `list-cfg`â€‹ ï¼Œå¯¹äºè¿™ä¸ªå·¥å…·è€Œè¨€ï¼Œå¯ä»¥é€‰æ‹©ä¸æ·»åŠ é¢å¤–çš„å‚æ•°ï¼Œå°±åƒä¸Šé¢çš„ä¸€æ ·ï¼Œè¿™æ ·å°±ä¼šå°†æ‰€æœ‰çš„é…ç½®æ–‡ä»¶éƒ½æ‰“å°å‡ºæ¥ã€‚é‚£åŒæ—¶ä¹Ÿå¯ä»¥åŠ ä¸Šä¸€ä¸ªå‚æ•° `-p`â€‹ æˆ– `--pattern`â€‹ ï¼Œåé¢è¾“å…¥çš„å†…å®¹å°†ä¼šåœ¨æ‰€æœ‰çš„ config æ–‡ä»¶é‡Œè¿›è¡Œæ¨¡ç³ŠåŒ¹é…æœç´¢ï¼Œç„¶åè¿”å›æœ€æœ‰å¯èƒ½å¾—å†…å®¹ã€‚æˆ‘ä»¬å¯ä»¥ç”¨æ¥æœç´¢ç‰¹å®šæ¨¡å‹çš„é…ç½®æ–‡ä»¶ï¼Œæ¯”å¦‚ä¾‹å­ä¸­çš„ internlm2_1_8b ,ä¹Ÿå¯ä»¥ç”¨æ¥æœç´¢åƒæ˜¯å¾®è°ƒæ–¹æ³• qlora ã€‚ æ ¹æ®ä¸Šé¢çš„å®šå‘æœç´¢æŒ‡ä»¤å¯ä»¥çœ‹åˆ°ç›®å‰åªæœ‰ä¸¤ä¸ªæ”¯æŒ internlm2-1.8B çš„æ¨¡å‹é…ç½®æ–‡ä»¶ã€‚
>
> â€

é…ç½®æ–‡ä»¶åçš„è§£é‡Šï¼š

è™½ç„¶æˆ‘ä»¬ç”¨çš„æ•°æ®é›†å¹¶ä¸æ˜¯ `alpaca`â€‹ è€Œæ˜¯æˆ‘ä»¬è‡ªå·±é€šè¿‡è„šæœ¬åˆ¶ä½œçš„å°åŠ©æ‰‹æ•°æ®é›† ï¼Œä½†æ˜¯ç”±äºæˆ‘ä»¬æ˜¯é€šè¿‡ `QLoRA`â€‹ çš„æ–¹å¼å¯¹ `internlm2-chat-1.8b`â€‹ è¿›è¡Œå¾®è°ƒã€‚è€Œæœ€ç›¸è¿‘çš„é…ç½®æ–‡ä»¶åº”è¯¥å°±æ˜¯ `internlm2_1_8b_qlora_alpaca_e3`â€‹ ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€‰æ‹©æ‹·è´è¿™ä¸ªé…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•ï¼š

```shell
# åˆ›å»ºä¸€ä¸ªå­˜æ”¾ config æ–‡ä»¶çš„æ–‡ä»¶å¤¹
mkdir -p /root/ft/config

# ä½¿ç”¨ XTuner ä¸­çš„ copy-cfg åŠŸèƒ½å°† config æ–‡ä»¶å¤åˆ¶åˆ°æŒ‡å®šçš„ä½ç½®
xtuner copy-cfg internlm2_1_8b_qlora_alpaca_e3 /root/ft/config
```

> è¿™é‡Œæˆ‘ä»¬å°±ç”¨åˆ°äº† XTuner å·¥å…·ç®±ä¸­çš„ç¬¬äºŒä¸ªå·¥å…· `copy-cfg`â€‹ ï¼Œè¯¥å·¥å…·æœ‰ä¸¤ä¸ªå¿…é¡»è¦å¡«å†™çš„å‚æ•° `{CONFIG_NAME}`â€‹ å’Œ `{SAVE_PATH}`â€‹ ï¼Œåœ¨æˆ‘ä»¬çš„è¾“å…¥çš„è¿™ä¸ªæŒ‡ä»¤ä¸­ï¼Œæˆ‘ä»¬çš„ `{CONFIG_NAME}`â€‹ å¯¹åº”çš„æ˜¯ä¸Šé¢æœç´¢åˆ°çš„ `internlm2_1_8b_qlora_alpaca_e3`â€‹ ,è€Œ `{SAVE_PATH}`â€‹ åˆ™å¯¹åº”çš„æ˜¯åˆšåˆšæ–°å»ºçš„ `/root/ft/config`â€‹ã€‚æˆ‘ä»¬å‡å¦‚éœ€è¦å¤åˆ¶å…¶ä»–çš„é…ç½®æ–‡ä»¶åªéœ€è¦ä¿®æ”¹è¿™ä¸¤ä¸ªå‚æ•°å³å¯å®ç°ã€‚ è¾“å…¥åæˆ‘ä»¬å°±èƒ½å¤Ÿçœ‹åˆ°åœ¨æˆ‘ä»¬çš„ `/root/ft/config`â€‹ æ–‡ä»¶å¤¹ä¸‹æœ‰ä¸€ä¸ªåä¸º `internlm2_1_8b_qlora_alpaca_e3_copy.py`â€‹ çš„æ–‡ä»¶äº†ã€‚

```
|-- config/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
```

#### 2.2.4 å°ç»“

ç»è¿‡äº†ä»¥ä¸Šçš„æ­¥éª¤åï¼Œæˆ‘ä»¬çš„ `ft`â€‹ æ–‡ä»¶å¤¹é‡Œåº”è¯¥æ˜¯è¿™æ ·çš„ï¼š

```
|-- ft/
    |-- config/
        |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- model/
        |-- tokenizer.model
        |-- config.json
        |-- tokenization_internlm2.py
        |-- model-00002-of-00002.safetensors
        |-- tokenizer_config.json
        |-- model-00001-of-00002.safetensors
        |-- model.safetensors.index.json
        |-- configuration.json
        |-- special_tokens_map.json
        |-- modeling_internlm2.py
        |-- README.md
        |-- configuration_internlm2.py
        |-- generation_config.json
        |-- tokenization_internlm2_fast.py
    |-- data/
        |-- personal_assistant.json
        |-- generate_data.py
```

æ˜¯ä¸æ˜¯æ„Ÿè§‰å…¶å®å¾®è°ƒä¹Ÿä¸è¿‡å¦‚æ­¤ï¼äº‹å®ä¸Šç¡®å®æ˜¯è¿™æ ·çš„ï¼å…¶å®åœ¨å¾®è°ƒçš„æ—¶å€™æœ€é‡è¦çš„è¿˜æ˜¯è¦è‡ªå·±å‡†å¤‡ä¸€ä»½é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œè¿™ä¸ªæ‰æ˜¯ä½ èƒ½å¦çœŸå¾®è°ƒå‡ºæ•ˆæœæœ€æ ¸å¿ƒçš„åˆ©å™¨ã€‚

å¾®è°ƒä¹Ÿç»å¸¸è¢«æˆç§°ä¸ºæ˜¯ç‚¼ä¸¹ï¼Œå°±æ˜¯è¯´ä½ ç‚¼ä¸¹çš„æ—¶å€™ä½ å¾—æ€è€ƒå¥½ç”¨ä»€ä¹ˆæ ·çš„ææ–™ã€ç”¨å¤šå¤§çš„ç«å€™ã€çƒ¤å¤šä¹…çš„æ—¶é—´ä»¥åŠç”¨ä»€ä¹ˆä¸¹ç‚‰å»çƒ§ã€‚è¿™é‡Œçš„ä¸¹ç‚‰å…¶å®æˆ‘ä»¬å¯ä»¥æƒ³è±¡ä¸º XTuner ï¼Œåªè¦ä¸¹ç‚‰çš„è´¨é‡è¿‡å¾—å»ï¼Œç‚¼ä¸¹çš„æ—¶å€™ä¸ä¼šç‚¸ï¼Œä¸€èˆ¬éƒ½æ˜¯æ²¡é—®é¢˜çš„ã€‚ä½†æ˜¯å‡å¦‚ç‚¼ä¸¹çš„ææ–™ï¼ˆå°±æ˜¯æ•°æ®é›†ï¼‰æœ¬æ¥å°±æ˜¯åƒåœ¾ï¼Œé‚£æ— è®ºæ€ä¹ˆç‚¼ï¼ˆå¾®è°ƒå‚æ•°çš„è°ƒæ•´ï¼‰ï¼Œç‚¼å¤šä¹…ï¼ˆè®­ç»ƒçš„è½®æ•°ï¼‰ï¼Œç‚¼å‡ºæ¥çš„ä¸œè¥¿è¿˜åªèƒ½ä¸”åªä¼šæ˜¯åƒåœ¾ã€‚åªæœ‰è¯´ç”¨äº†æ¯”è¾ƒå¥½çš„ææ–™ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥è€ƒè™‘è¯´è¦ç‚¼å¤šä¹…ä»¥åŠç”¨ä»€ä¹ˆåŠæ³•å»ç‚¼çš„é—®é¢˜ã€‚å› æ­¤æ€»çš„æ¥è¯´ï¼Œå­¦ä¼šå¦‚ä½•æ„å»ºä¸€ä»½é«˜è´¨é‡çš„æ•°æ®é›†æ˜¯è‡³å…³é‡è¦çš„ã€‚

â€

### 2.3 é…ç½®æ–‡ä»¶ä¿®æ”¹

åœ¨é€‰æ‹©äº†ä¸€ä¸ªæœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶å¹¶å‡†å¤‡å¥½å…¶ä»–å†…å®¹åï¼Œä¸‹é¢æˆ‘ä»¬è¦åšçš„äº‹æƒ…å°±æ˜¯æ ¹æ®æˆ‘ä»¬è‡ªå·±çš„å†…å®¹å¯¹è¯¥é…ç½®æ–‡ä»¶è¿›è¡Œè°ƒæ•´ï¼Œä½¿å…¶èƒ½å¤Ÿæ»¡è¶³æˆ‘ä»¬å®é™…è®­ç»ƒçš„è¦æ±‚ã€‚

* é…ç½®æ–‡ä»¶ä»‹ç»

é€šè¿‡æŠ˜å éƒ¨åˆ†çš„ä¿®æ”¹ï¼Œå†…å®¹å¦‚ä¸‹ï¼Œå¯ä»¥ç›´æ¥å°†ä»¥ä¸‹ä»£ç å¤åˆ¶åˆ° `/root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py`â€‹ æ–‡ä»¶ä¸­ï¼ˆå…ˆ `Ctrl + A`â€‹ é€‰ä¸­æ‰€æœ‰æ–‡ä»¶å¹¶åˆ é™¤åå†å°†ä»£ç å¤åˆ¶è¿›å»ï¼‰ã€‚

* å‚æ•°ä¿®æ”¹ç»†èŠ‚

â€

```python
# Copyright (c) OpenMMLab. All rights reserved.
import torch
from datasets import load_dataset
from mmengine.dataset import DefaultSampler
from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,
                            LoggerHook, ParamSchedulerHook)
from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR, LinearLR
from peft import LoraConfig
from torch.optim import AdamW
from transformers import (AutoModelForCausalLM, AutoTokenizer,
                          BitsAndBytesConfig)

from xtuner.dataset import process_hf_dataset
from xtuner.dataset.collate_fns import default_collate_fn
from xtuner.dataset.map_fns import openai_map_fn, template_map_fn_factory
from xtuner.engine.hooks import (DatasetInfoHook, EvaluateChatHook,
                                 VarlenAttnArgsToMessageHubHook)
from xtuner.engine.runner import TrainLoop
from xtuner.model import SupervisedFinetune
from xtuner.parallel.sequence import SequenceParallelSampler
from xtuner.utils import PROMPT_TEMPLATE, SYSTEM_TEMPLATE

#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
pretrained_model_name_or_path = '/root/ft/model'
use_varlen_attn = False

# Data
alpaca_en_path = '/root/ft/data/personal_assistant.json'
prompt_template = PROMPT_TEMPLATE.default
max_length = 1024
pack_to_max_length = True

# parallel
sequence_parallel_size = 1

# Scheduler & Optimizer
batch_size = 1  # per_device
accumulative_counts = 16
accumulative_counts *= sequence_parallel_size
dataloader_num_workers = 0
max_epochs = 2
optim_type = AdamW
lr = 2e-4
betas = (0.9, 0.999)
weight_decay = 0
max_norm = 1  # grad clip
warmup_ratio = 0.03

# Save
save_steps = 300
save_total_limit = 3  # Maximum checkpoints to keep (-1 means unlimited)

# Evaluate the generation performance during the training
evaluation_freq = 300
SYSTEM = ''
evaluation_inputs = ['è¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'ä½ æ˜¯è°', 'ä½ æ˜¯æˆ‘çš„å°åŠ©æ‰‹å—']

#######################################################################
#                      PART 2  Model & Tokenizer                      #
#######################################################################
tokenizer = dict(
    type=AutoTokenizer.from_pretrained,
    pretrained_model_name_or_path=pretrained_model_name_or_path,
    trust_remote_code=True,
    padding_side='right')

model = dict(
    type=SupervisedFinetune,
    use_varlen_attn=use_varlen_attn,
    llm=dict(
        type=AutoModelForCausalLM.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        quantization_config=dict(
            type=BitsAndBytesConfig,
            load_in_4bit=True,
            load_in_8bit=False,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type='nf4')),
    lora=dict(
        type=LoraConfig,
        r=64,
        lora_alpha=16,
        lora_dropout=0.1,
        bias='none',
        task_type='CAUSAL_LM'))

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
alpaca_en = dict(
    type=process_hf_dataset,
    dataset=dict(type=load_dataset, path='json', data_files=dict(train=alpaca_en_path)),
    tokenizer=tokenizer,
    max_length=max_length,
    dataset_map_fn=openai_map_fn,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length,
    use_varlen_attn=use_varlen_attn)

sampler = SequenceParallelSampler \
    if sequence_parallel_size > 1 else DefaultSampler
train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=alpaca_en,
    sampler=dict(type=sampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn, use_varlen_attn=use_varlen_attn))

#######################################################################
#                    PART 4  Scheduler & Optimizer                    #
#######################################################################
# optimizer
optim_wrapper = dict(
    type=AmpOptimWrapper,
    optimizer=dict(
        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),
    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),
    accumulative_counts=accumulative_counts,
    loss_scale='dynamic',
    dtype='float16')

# learning policy
# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501
param_scheduler = [
    dict(
        type=LinearLR,
        start_factor=1e-5,
        by_epoch=True,
        begin=0,
        end=warmup_ratio * max_epochs,
        convert_to_iter_based=True),
    dict(
        type=CosineAnnealingLR,
        eta_min=0.0,
        by_epoch=True,
        begin=warmup_ratio * max_epochs,
        end=max_epochs,
        convert_to_iter_based=True)
]

# train, val, test setting
train_cfg = dict(type=TrainLoop, max_epochs=max_epochs)

#######################################################################
#                           PART 5  Runtime                           #
#######################################################################
# Log the dialogue periodically during the training process, optional
custom_hooks = [
    dict(type=DatasetInfoHook, tokenizer=tokenizer),
    dict(
        type=EvaluateChatHook,
        tokenizer=tokenizer,
        every_n_iters=evaluation_freq,
        evaluation_inputs=evaluation_inputs,
        system=SYSTEM,
        prompt_template=prompt_template)
]

if use_varlen_attn:
    custom_hooks += [dict(type=VarlenAttnArgsToMessageHubHook)]

# configure default hooks
default_hooks = dict(
    # record the time of every iteration.
    timer=dict(type=IterTimerHook),
    # print log every 10 iterations.
    logger=dict(type=LoggerHook, log_metric_by_epoch=False, interval=10),
    # enable the parameter scheduler.
    param_scheduler=dict(type=ParamSchedulerHook),
    # save checkpoint per `save_steps`.
    checkpoint=dict(
        type=CheckpointHook,
        by_epoch=False,
        interval=save_steps,
        max_keep_ckpts=save_total_limit),
    # set sampler seed in distributed evrionment.
    sampler_seed=dict(type=DistSamplerSeedHook),
)

# configure environment
env_cfg = dict(
    # whether to enable cudnn benchmark
    cudnn_benchmark=False,
    # set multi process parameters
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    # set distributed parameters
    dist_cfg=dict(backend='nccl'),
)

# set visualizer
visualizer = None

# set log level
log_level = 'INFO'

# load from which checkpoint
load_from = None

# whether to resume training from the loaded checkpoint
resume = False

# Defaults to use random seed and disable `deterministic`
randomness = dict(seed=None, deterministic=False)

# set log processor
log_processor = dict(by_epoch=False)
```

è¿™ä¸€èŠ‚æˆ‘ä»¬è®²è¿°äº†å¾®è°ƒè¿‡ç¨‹ä¸­ä¸€äº›å¸¸è§çš„éœ€è¦è°ƒæ•´çš„å†…å®¹ï¼ŒåŒ…æ‹¬å„ç§çš„è·¯å¾„ã€è¶…å‚æ•°ã€è¯„ä¼°é—®é¢˜ç­‰ç­‰ã€‚å®Œæˆäº†è¿™éƒ¨åˆ†çš„ä¿®æ”¹åï¼Œæˆ‘ä»¬å°±å¯ä»¥æ­£å¼çš„å¼€å§‹æˆ‘ä»¬ä¸‹ä¸€é˜¶æ®µçš„æ—…ç¨‹ï¼š XTuner å¯åŠ¨~ï¼

â€

### 2.4 æ¨¡å‹è®­ç»ƒ

#### 2.4.1 å¸¸è§„è®­ç»ƒ

å½“æˆ‘ä»¬å‡†å¤‡å¥½äº†é…ç½®æ–‡ä»¶å¥½ï¼Œæˆ‘ä»¬åªéœ€è¦å°†ä½¿ç”¨ `xtuner train`â€‹ æŒ‡ä»¤å³å¯å¼€å§‹è®­ç»ƒã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ  `--work-dir`â€‹ æŒ‡å®šç‰¹å®šçš„æ–‡ä»¶ä¿å­˜ä½ç½®ï¼Œæ¯”å¦‚è¯´å°±ä¿å­˜åœ¨ `/root/ft/train`â€‹ è·¯å¾„ä¸‹ã€‚å‡å¦‚ä¸æ·»åŠ çš„è¯æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹æ–‡ä»¶å°†é»˜è®¤ä¿å­˜åœ¨ `./work_dirs/internlm2_1_8b_qlora_alpaca_e3_copy`â€‹ çš„ä½ç½®ï¼Œå°±æ¯”å¦‚è¯´æˆ‘æ˜¯åœ¨ `/root/ft/train`â€‹ çš„è·¯å¾„ä¸‹è¾“å…¥è¯¥æŒ‡ä»¤ï¼Œé‚£ä¹ˆæˆ‘çš„æ–‡ä»¶ä¿å­˜çš„ä½ç½®å°±æ˜¯åœ¨ `/root/ft/train/work_dirs/internlm2_1_8b_qlora_alpaca_e3_copy`â€‹ çš„ä½ç½®ä¸‹ã€‚

```shell
# æŒ‡å®šä¿å­˜è·¯å¾„
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train
```

â€‹![image](assets/image-20240419021430-afgs6rn.png)â€‹

åœ¨è¾“å…¥è®­ç»ƒå®Œåçš„æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š

```
|-- train/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- iter_600.pth
    |-- last_checkpoint
    |-- iter_768.pth
    |-- iter_300.pth
    |-- 20240406_203957/
        |-- 20240406_203957.log
        |-- vis_data/
            |-- 20240406_203957.json
            |-- eval_outputs_iter_599.txt
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- eval_outputs_iter_299.txt
            |-- config.py
```

#### 2.4.2 ä½¿ç”¨ deepspeed æ¥åŠ é€Ÿè®­ç»ƒ

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç»“åˆ XTuner å†…ç½®çš„ `deepspeed`â€‹ æ¥åŠ é€Ÿæ•´ä½“çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå…±æœ‰ä¸‰ç§ä¸åŒçš„ `deepspeed`â€‹ ç±»å‹å¯è¿›è¡Œé€‰æ‹©ï¼Œåˆ†åˆ«æ˜¯ `deepspeed_zero1`â€‹, `deepspeed_zero2`â€‹ å’Œ `deepspeed_zero3`â€‹ï¼ˆè¯¦ç»†çš„ä»‹ç»å¯çœ‹ä¸‹æ‹‰æ¡†ï¼‰ã€‚

DeepSpeedä¼˜åŒ–å™¨åŠå…¶é€‰æ‹©æ–¹æ³•

```shell
# ä½¿ç”¨ deepspeed æ¥åŠ é€Ÿè®­ç»ƒ
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train_deepspeed --deepspeed deepspeed_zero2
```

å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡ `deepspeed`â€‹ æ¥è®­ç»ƒåå¾—åˆ°çš„æƒé‡æ–‡ä»¶å’ŒåŸæœ¬çš„æƒé‡æ–‡ä»¶æ˜¯æœ‰æ‰€å·®åˆ«çš„ï¼ŒåŸæœ¬çš„ä»…ä»…æ˜¯ä¸€ä¸ª .pth çš„æ–‡ä»¶ï¼Œè€Œä½¿ç”¨äº† `deepspeed`â€‹ åˆ™æ˜¯ä¸€ä¸ªåå­—å¸¦æœ‰ .pth çš„æ–‡ä»¶å¤¹ï¼Œåœ¨è¯¥æ–‡ä»¶å¤¹é‡Œä¿å­˜äº†ä¸¤ä¸ª .pt æ–‡ä»¶ã€‚å½“ç„¶è¿™ä¸¤è€…åœ¨å…·ä½“çš„ä½¿ç”¨ä¸Šå¹¶æ²¡æœ‰å¤ªå¤§çš„å·®åˆ«ï¼Œéƒ½æ˜¯å¯ä»¥è¿›è¡Œè½¬åŒ–å¹¶æ•´åˆã€‚

```
|-- train_deepspeed/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- zero_to_fp32.py
    |-- last_checkpoint
    |-- iter_600.pth/
        |-- bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
        |-- mp_rank_00_model_states.pt
    |-- 20240406_220727/
        |-- 20240406_220727.log
        |-- vis_data/
            |-- 20240406_220727.json
            |-- eval_outputs_iter_599.txt
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- eval_outputs_iter_299.txt
            |-- config.py
    |-- iter_768.pth/
        |-- bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
        |-- mp_rank_00_model_states.pt
    |-- iter_300.pth/
        |-- bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
        |-- mp_rank_00_model_states.pt
```

#### 2.4.3 è®­ç»ƒç»“æœ

ä½†æ˜¯å…¶å®æ— è®ºæ˜¯ç”¨å“ªç§æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°çš„ç»“æœéƒ½æ˜¯å¤§å·®ä¸å·®çš„ã€‚æˆ‘ä»¬ç”±äºè®¾ç½®äº†300è½®è¯„ä¼°ä¸€æ¬¡ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å¯¹æ¯”ä¸€ä¸‹300è½®å’Œ600è½®çš„è¯„ä¼°é—®é¢˜ç»“æœæ¥çœ‹çœ‹å·®åˆ«ã€‚

â€

â€‹![image](assets/image-20240419021834-ohjobcj.png)â€‹

â€

â€‹![image](assets/image-20240419022059-64lu794.png)â€‹

â€

â€‹![image](assets/image-20240419022904-qvd4e5y.png)â€‹

â€‹![image](assets/image-20240419022917-iaa2l9p.png)â€‹

é€šè¿‡ä¸¤è€…çš„å¯¹æ¯”æˆ‘ä»¬å…¶å®å°±å¯ä»¥å¾ˆæ¸…æ¥šçš„çœ‹åˆ°ï¼Œåœ¨300è½®çš„æ—¶å€™æ¨¡å‹å·²ç»å­¦ä¼šäº†åœ¨æˆ‘é—® â€œä½ æ˜¯è°â€ æˆ–è€…è¯´ â€œè¯·ä½ ä»‹ç»ä¸€ä¸‹æˆ‘è‡ªå·±â€ çš„æ—¶å€™å›ç­” â€œæˆ‘æ˜¯ä¸šè¥é”€åŠ©æ‰‹çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦â€ã€‚

ä½†æ˜¯ä¸¤è€…çš„ä¸åŒæ˜¯åœ¨è¯¢é—® â€œä½ æ˜¯æˆ‘çš„å°åŠ©æ‰‹â€ çš„è¿™ä¸ªé—®é¢˜ä¸Šï¼Œ300è½®çš„æ—¶å€™æ˜¯å›ç­”æ­£ç¡®çš„ï¼Œå›ç­”äº† â€œæ˜¯â€ ï¼Œä½†æ˜¯åœ¨600è½®çš„æ—¶å€™å›ç­”çš„è¿˜æ˜¯ â€œæˆ‘æ˜¯ä¸šè¥é”€åŠ©æ‰‹çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦â€ è¿™ä¸€æ®µè¯ã€‚è¿™è¡¨æ˜æ¨¡å‹åœ¨ç¬¬ä¸€æ‰¹æ¬¡ç¬¬600è½®çš„æ—¶å€™å·²ç»å‡ºç°ä¸¥é‡çš„è¿‡æ‹Ÿåˆï¼ˆå³æ¨¡å‹ä¸¢å¤±äº†åŸºç¡€çš„èƒ½åŠ›ï¼Œåªä¼šæˆä¸ºæŸä¸€å¥è¯çš„å¤è¯»æœºï¼‰ç°è±¡äº†ï¼Œåˆ°åé¢çš„è¯æ— è®ºæˆ‘ä»¬å†é—®ä»€ä¹ˆï¼Œå¾—åˆ°çš„ç»“æœä¹Ÿå°±åªèƒ½æ˜¯å›ç­”è¿™ä¸€å¥è¯äº†ï¼Œæ¨¡å‹å·²ç»ä¸ä¼šå†è¯´åˆ«çš„è¯äº†ã€‚å› æ­¤å‡å¦‚ä»¥é€šç”¨èƒ½åŠ›çš„è§’åº¦é€‰æ‹©æœ€åˆé€‚çš„æƒé‡æ–‡ä»¶çš„è¯æˆ‘ä»¬å¯èƒ½ä¼šé€‰æ‹©å‰é¢çš„æƒé‡æ–‡ä»¶è¿›è¡Œåç»­çš„æ¨¡å‹è½¬åŒ–åŠæ•´åˆå·¥ä½œã€‚

å‡å¦‚æˆ‘ä»¬æƒ³è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå…¶å®å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ–¹å¼è§£å†³ï¼š

1. **å‡å°‘ä¿å­˜æƒé‡æ–‡ä»¶çš„é—´éš”å¹¶å¢åŠ æƒé‡æ–‡ä»¶ä¿å­˜çš„ä¸Šé™**ï¼šè¿™ä¸ªæ–¹æ³•å®é™…ä¸Šå°±æ˜¯é€šè¿‡é™ä½é—´éš”ç»“åˆè¯„ä¼°é—®é¢˜çš„ç»“æœï¼Œä»è€Œæ‰¾åˆ°æœ€ä¼˜çš„æƒé‡æ–‡ã€‚æˆ‘ä»¬å¯ä»¥æ¯éš”100ä¸ªæ‰¹æ¬¡æ¥çœ‹ä»€ä¹ˆæ—¶å€™æ¨¡å‹å·²ç»å­¦åˆ°äº†è¿™éƒ¨åˆ†çŸ¥è¯†ä½†æ˜¯è¿˜ä¿ç•™ç€åŸºæœ¬çš„å¸¸è¯†ï¼Œä»€ä¹ˆæ—¶å€™å·²ç»è¿‡æ‹Ÿåˆä¸¥é‡åªä¼šè¯´ä¸€å¥è¯äº†ã€‚ä½†æ˜¯ç”±äºå†é…ç½®æ–‡ä»¶æœ‰è®¾ç½®æƒé‡æ–‡ä»¶ä¿å­˜æ•°é‡çš„ä¸Šé™ï¼Œå› æ­¤åŒæ—¶å°†è¿™ä¸ªä¸Šé™åŠ å¤§ä¹Ÿæ˜¯éå¸¸å¿…è¦çš„ã€‚
2. **å¢åŠ å¸¸è§„çš„å¯¹è¯æ•°æ®é›†ä»è€Œç¨€é‡ŠåŸæœ¬æ•°æ®çš„å æ¯”**ï¼šè¿™ä¸ªæ–¹æ³•å…¶å®å°±æ˜¯å¸Œæœ›æˆ‘ä»¬æ­£å¸¸ç”¨å¯¹è¯æ•°æ®é›†åšæŒ‡ä»¤å¾®è°ƒçš„åŒæ—¶è¿˜åŠ ä¸Šä¸€éƒ¨åˆ†çš„æ•°æ®é›†æ¥è®©æ¨¡å‹æ—¢èƒ½å¤Ÿå­¦åˆ°æ­£å¸¸å¯¹è¯ï¼Œä½†æ˜¯åœ¨é‡åˆ°ç‰¹å®šé—®é¢˜æ—¶è¿›è¡Œç‰¹æ®ŠåŒ–å¤„ç†ã€‚æ¯”å¦‚è¯´æˆ‘åœ¨ä¸€ä¸‡æ¡æ­£å¸¸çš„å¯¹è¯æ•°æ®é‡Œæ··å…¥ä¸¤åƒæ¡å’Œå°åŠ©æ‰‹ç›¸å…³çš„æ•°æ®é›†ï¼Œè¿™æ ·æ¨¡å‹åŒæ ·å¯ä»¥åœ¨ä¸ä¸¢å¤±å¯¹è¯èƒ½åŠ›çš„å‰æä¸‹å­¦åˆ°å‰‘é”‹å¤§ä½¬çš„å°åŠ©æ‰‹è¿™å¥è¯ã€‚è¿™ç§å…¶å®æ˜¯æ¯”è¾ƒå¸¸è§çš„å¤„ç†æ–¹å¼ï¼Œå¤§å®¶å¯ä»¥è‡ªå·±åŠ¨æ‰‹å°è¯•å®è·µä¸€ä¸‹ã€‚

> å¦å¤–å‡å¦‚æˆ‘ä»¬æ¨¡å‹ä¸­é€”ä¸­æ–­äº†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å‚è€ƒä»¥ä¸‹æ–¹æ³•å®ç°æ¨¡å‹ç»­è®­å·¥ä½œ

* æ¨¡å‹ç»­è®­æŒ‡å—

å‡å¦‚æˆ‘ä»¬çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çªç„¶è¢«ä¸­æ–­äº†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡åœ¨åŸæœ‰æŒ‡ä»¤çš„åŸºç¡€ä¸ŠåŠ ä¸Š `--resume {checkpoint_path}`â€‹ æ¥å®ç°æ¨¡å‹çš„ç»§ç»­è®­ç»ƒã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªç»§ç»­è®­ç»ƒå¾—åˆ°çš„æƒé‡æ–‡ä»¶å’Œä¸­æ–­å‰çš„å®Œå…¨ä¸€è‡´ï¼Œå¹¶ä¸ä¼šæœ‰ä»»ä½•åŒºåˆ«ã€‚ä¸‹é¢æˆ‘å°†ç”¨è®­ç»ƒäº†500è½®çš„ä¾‹å­æ¥è¿›è¡Œæ¼”ç¤ºã€‚

```shell
# æ¨¡å‹ç»­è®­
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train --resume /root/ft/train/iter_600.pth
```

åœ¨å®æµ‹è¿‡ç¨‹ä¸­ï¼Œè™½ç„¶æƒé‡æ–‡ä»¶å¹¶æ²¡æœ‰å‘ç”Ÿæ”¹å˜ï¼Œä½†æ˜¯ä¼šå¤šä¸€ä¸ªä»¥æ—¶é—´æˆ³ä¸ºåçš„è®­ç»ƒè¿‡ç¨‹æ–‡ä»¶å¤¹ä¿å­˜è®­ç»ƒçš„è¿‡ç¨‹æ•°æ®ã€‚

```
|-- train/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- iter_600.pth
    |-- last_checkpoint
    |-- iter_768.pth
    |-- iter_300.pth
    |-- 20240406_203957/
        |-- 20240406_203957.log
        |-- vis_data/
            |-- 20240406_203957.json
            |-- eval_outputs_iter_599.txt
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- eval_outputs_iter_299.txt
            |-- config.py
    |-- 20240406_225723/
        |-- 20240406_225723.log
        |-- vis_data/
            |-- 20240406_225723.json
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- config.py
```

â€

#### 2.4.4 å°ç»“

è®²è§£æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç§ç§ç»†èŠ‚å†…å®¹ï¼ŒåŒ…æ‹¬äº†æ¨¡å‹è®­ç»ƒä¸­çš„å„ä¸ªå‚æ•°ä»¥ã€æƒé‡æ–‡ä»¶çš„é€‰æ‹©æ–¹å¼ä»¥åŠæ¨¡å‹ç»­è®­çš„æ–¹æ³•ã€‚å¯ä»¥çœ‹åˆ°æ˜¯å¦ä½¿ç”¨ `--work-dir`â€‹ å’Œ æ˜¯å¦ä½¿ç”¨ `--deepspeed`â€‹ ä¼šå¯¹æ–‡ä»¶çš„ä¿å­˜ä½ç½®ä»¥åŠæƒé‡æ–‡ä»¶çš„ä¿å­˜æ–¹å¼æœ‰æ‰€ä¸åŒï¼Œå¤§å®¶ä¹Ÿå¯ä»¥é€šè¿‡å®è·µå»å®é™…çš„æµ‹è¯•æ„Ÿå—ä¸€ä¸‹ã€‚é‚£ä¹ˆåœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠè®­ç»ƒå¾—åˆ°çš„ .pth æ–‡ä»¶è¿›è¡Œä¸‹ä¸€æ­¥çš„è½¬æ¢å’Œæ•´åˆå·¥ä½œäº†ï¼

â€

### 2.5 æ¨¡å‹è½¬æ¢ã€æ•´åˆã€æµ‹è¯•åŠéƒ¨ç½²

#### 2.5.1 æ¨¡å‹è½¬æ¢

æ¨¡å‹è½¬æ¢çš„æœ¬è´¨å…¶å®å°±æ˜¯å°†åŸæœ¬ä½¿ç”¨ Pytorch è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºç›®å‰é€šç”¨çš„ Huggingface æ ¼å¼æ–‡ä»¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æŒ‡ä»¤æ¥å®ç°ä¸€é”®è½¬æ¢ã€‚

> æ³¨æ„ï¼š${æƒé‡æ–‡ä»¶åœ°å€} /root/ft/train/iter_300.pth  æ˜¯è®­ç»ƒç¬¬300è½®çš„æ¨¡å‹æ–‡ä»¶ï¼Œå®é™…ä¸Šå¯èƒ½æ¯ä¸ªäººæ•°æ®é›†é•¿åº¦ä¸åŒæ‰€ä»¥è®­ç»ƒæœ€ç»ˆè½®æ•°ä¸åŒï¼Œæ‰€ä»¥æŒ‰ç…§è‡ªå·±çš„æ¥ï¼Œ300è½®ä¼šå¥½ä¸€äº›ï¼Œå› ä¸ºæ²¡æœ‰è¿‡æ‹Ÿåˆã€‚

â€‹![image](assets/image-20240419182120-tqof7cu.png)â€‹

```shell
# åˆ›å»ºä¸€ä¸ªä¿å­˜è½¬æ¢å Huggingface æ ¼å¼çš„æ–‡ä»¶å¤¹
mkdir -p /root/ft/huggingface

# æ¨¡å‹è½¬æ¢
# xtuner convert pth_to_hf ${é…ç½®æ–‡ä»¶åœ°å€} ${æƒé‡æ–‡ä»¶åœ°å€} ${è½¬æ¢åæ¨¡å‹ä¿å­˜åœ°å€}
xtuner convert pth_to_hf /root/ft/train/internlm2_1_8b_qlora_alpaca_e3_copy.py /root/ft/train/iter_300.pth /root/ft/huggingface
```

å¤§æ¦‚ä¼šèŠ±5åˆ†é’Ÿ

è½¬åŒ–å®Œæˆï¼š

â€‹![image](assets/image-20240419191029-bum5l7e.png)â€‹

â€

è½¬æ¢å®Œæˆåï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹è¢«è½¬æ¢ä¸º Huggingface ä¸­å¸¸ç”¨çš„ .bin æ ¼å¼æ–‡ä»¶ï¼Œè¿™å°±ä»£è¡¨ç€æ–‡ä»¶æˆåŠŸè¢«è½¬åŒ–ä¸º Huggingface æ ¼å¼äº†ã€‚

â€‹![image](assets/image-20240419183451-2l36l6j.png)â€‹

**æ­¤æ—¶ï¼Œhuggingface æ–‡ä»¶å¤¹å³ä¸ºæˆ‘ä»¬å¹³æ—¶æ‰€ç†è§£çš„æ‰€è°“ â€œLoRA æ¨¡å‹æ–‡ä»¶â€**

> å¯ä»¥ç®€å•ç†è§£ï¼šLoRA æ¨¡å‹æ–‡ä»¶ = Adapter

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬å…¶å®è¿˜å¯ä»¥åœ¨è½¬æ¢çš„æŒ‡ä»¤ä¸­æ·»åŠ å‡ ä¸ªé¢å¤–çš„å‚æ•°ï¼ŒåŒ…æ‹¬ä»¥ä¸‹ä¸¤ä¸ªï¼š

|å‚æ•°å|è§£é‡Š|
| -----------------------| ----------------------------------------------|
|--fp32|ä»£è¡¨ä»¥fp32çš„ç²¾åº¦å¼€å¯ï¼Œå‡å¦‚ä¸è¾“å…¥åˆ™é»˜è®¤ä¸ºfp16|
|--max-shard-size {GB}|ä»£è¡¨æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§çš„å¤§å°ï¼ˆé»˜è®¤ä¸º2GBï¼‰|

å‡å¦‚æœ‰ç‰¹å®šçš„éœ€è¦ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸Šé¢çš„è½¬æ¢æŒ‡ä»¤åè¿›è¡Œæ·»åŠ ã€‚ç”±äºæœ¬æ¬¡æµ‹è¯•çš„æ¨¡å‹æ–‡ä»¶è¾ƒå°ï¼Œå¹¶ä¸”å·²ç»éªŒè¯è¿‡æ‹Ÿåˆï¼Œæ•…æ²¡æœ‰æ·»åŠ ã€‚å‡å¦‚åŠ ä¸Šçš„è¯åº”è¯¥æ˜¯è¿™æ ·çš„ï¼š

```shell
xtuner convert pth_to_hf /root/ft/train/internlm2_1_8b_qlora_alpaca_e3_copy.py /root/ft/train/iter_768.pth /root/ft/huggingface --fp32 --max-shard-size 2GB
```

â€

#### 2.5.2 æ¨¡å‹æ•´åˆ

æˆ‘ä»¬é€šè¿‡è§†é¢‘è¯¾ç¨‹çš„å­¦ä¹ å¯ä»¥äº†è§£åˆ°ï¼Œå¯¹äº LoRA æˆ–è€… QLoRA å¾®è°ƒå‡ºæ¥çš„æ¨¡å‹å…¶å®å¹¶ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªé¢å¤–çš„å±‚ï¼ˆadapterï¼‰ã€‚é‚£ä¹ˆè®­ç»ƒå®Œçš„è¿™ä¸ªå±‚æœ€ç»ˆè¿˜æ˜¯è¦ä¸åŸæ¨¡å‹è¿›è¡Œç»„åˆæ‰èƒ½è¢«æ­£å¸¸çš„ä½¿ç”¨ã€‚

è€Œå¯¹äºå…¨é‡å¾®è°ƒçš„æ¨¡å‹ï¼ˆfullï¼‰å…¶å®æ˜¯ä¸éœ€è¦è¿›è¡Œæ•´åˆè¿™ä¸€æ­¥çš„ï¼Œå› ä¸ºå…¨é‡å¾®è°ƒä¿®æ”¹çš„æ˜¯åŸæ¨¡å‹çš„æƒé‡è€Œéå¾®è°ƒä¸€ä¸ªæ–°çš„ adapter ï¼Œå› æ­¤æ˜¯ä¸éœ€è¦è¿›è¡Œæ¨¡å‹æ•´åˆçš„ã€‚

åœ¨ XTuner ä¸­ä¹Ÿæ˜¯æä¾›äº†ä¸€é”®æ•´åˆçš„æŒ‡ä»¤ï¼Œä½†æ˜¯åœ¨ä½¿ç”¨å‰æˆ‘ä»¬éœ€è¦å‡†å¤‡å¥½ä¸‰ä¸ªåœ°å€ï¼ŒåŒ…æ‹¬åŸæ¨¡å‹çš„åœ°å€ã€è®­ç»ƒå¥½çš„ adapter å±‚çš„åœ°å€ï¼ˆè½¬ä¸º Huggingface æ ¼å¼åä¿å­˜çš„éƒ¨åˆ†ï¼‰ä»¥åŠæœ€ç»ˆä¿å­˜çš„åœ°å€ã€‚

```shell
# åˆ›å»ºä¸€ä¸ªåä¸º final_model çš„æ–‡ä»¶å¤¹å­˜å‚¨æ•´åˆåçš„æ¨¡å‹æ–‡ä»¶
mkdir -p /root/ft/final_model

# è§£å†³ä¸€ä¸‹çº¿ç¨‹å†²çªçš„ Bug 
export MKL_SERVICE_FORCE_INTEL=1

# è¿›è¡Œæ¨¡å‹æ•´åˆ
# xtuner convert merge  ${NAME_OR_PATH_TO_LLM} ${NAME_OR_PATH_TO_ADAPTER} ${SAVE_PATH} 
xtuner convert merge /root/ft/model /root/ft/huggingface /root/ft/final_model
```

é‚£é™¤äº†ä»¥ä¸Šçš„ä¸‰ä¸ªåŸºæœ¬å‚æ•°ä»¥å¤–ï¼Œå…¶å®åœ¨æ¨¡å‹æ•´åˆè¿™ä¸€æ­¥è¿˜æ˜¯å…¶ä»–å¾ˆå¤šçš„å¯é€‰å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š

|å‚æ•°å|è§£é‡Š|
| ------------------------| ----------------------------------------------------------------------------------|
|--max-shard-size {GB}|ä»£è¡¨æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§çš„å¤§å°ï¼ˆé»˜è®¤ä¸º2GBï¼‰|
|--device {device_name}|è¿™é‡ŒæŒ‡çš„å°±æ˜¯deviceçš„åç§°ï¼Œå¯é€‰æ‹©çš„æœ‰cudaã€cpuå’Œautoï¼Œé»˜è®¤ä¸ºcudaå³ä½¿ç”¨gpuè¿›è¡Œè¿ç®—|
|--is-clip|è¿™ä¸ªå‚æ•°ä¸»è¦ç”¨äºç¡®å®šæ¨¡å‹æ˜¯ä¸æ˜¯CLIPæ¨¡å‹ï¼Œå‡å¦‚æ˜¯çš„è¯å°±è¦åŠ ä¸Šï¼Œä¸æ˜¯å°±ä¸éœ€è¦æ·»åŠ |

> CLIPï¼ˆContrastive Languageâ€“Image Pre-trainingï¼‰æ¨¡å‹æ˜¯ OpenAI å¼€å‘çš„ä¸€ç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿç†è§£å›¾åƒå’Œæè¿°å®ƒä»¬çš„æ–‡æœ¬ä¹‹é—´çš„å…³ç³»ã€‚CLIP é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå­¦ä¹ å›¾åƒå’Œå¯¹åº”æ–‡æœ¬ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œå®ç°äº†å¯¹å›¾åƒå†…å®¹çš„ç†è§£å’Œåˆ†ç±»ï¼Œç”šè‡³èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒã€‚ åœ¨æ¨¡å‹æ•´åˆå®Œæˆåï¼Œæˆ‘ä»¬å°±å¯ä»¥çœ‹åˆ° final_model æ–‡ä»¶å¤¹é‡Œç”Ÿæˆäº†å’ŒåŸæ¨¡å‹æ–‡ä»¶å¤¹éå¸¸è¿‘ä¼¼çš„å†…å®¹ï¼ŒåŒ…æ‹¬äº†åˆ†è¯å™¨ã€æƒé‡æ–‡ä»¶ã€é…ç½®ä¿¡æ¯ç­‰ç­‰ã€‚å½“æˆ‘ä»¬æ•´åˆå®Œæˆåï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿæ­£å¸¸çš„è°ƒç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¯¹è¯æµ‹è¯•äº†ã€‚

æ•´åˆå®Œæˆï¼š

â€‹![image](assets/image-20240419185604-aqbze9d.png)â€‹

æ•´åˆå®Œæˆåå¯ä»¥æŸ¥çœ‹åœ¨ final_model æ–‡ä»¶å¤¹ä¸‹çš„å†…å®¹ã€‚

â€‹![image](assets/image-20240419185627-3ssw1dq.png)â€‹

#### 2.5.3 å¯¹è¯æµ‹è¯•

åœ¨ XTuner ä¸­ä¹Ÿç›´æ¥çš„æä¾›äº†ä¸€å¥—åŸºäº transformers çš„å¯¹è¯ä»£ç ï¼Œè®©æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨ç»ˆç«¯ä¸ Huggingface æ ¼å¼çš„æ¨¡å‹è¿›è¡Œå¯¹è¯æ“ä½œã€‚æˆ‘ä»¬åªéœ€è¦å‡†å¤‡æˆ‘ä»¬åˆšåˆšè½¬æ¢å¥½çš„æ¨¡å‹è·¯å¾„å¹¶é€‰æ‹©å¯¹åº”çš„æç¤ºè¯æ¨¡ç‰ˆï¼ˆprompt-templateï¼‰å³å¯è¿›è¡Œå¯¹è¯ã€‚å‡å¦‚ prompt-template é€‰æ‹©æœ‰è¯¯ï¼Œå¾ˆæœ‰å¯èƒ½å¯¼è‡´æ¨¡å‹æ— æ³•æ­£ç¡®çš„è¿›è¡Œå›å¤ã€‚

> æƒ³è¦äº†è§£å…·ä½“æ¨¡å‹çš„ prompt-template æˆ–è€… XTuner é‡Œæ”¯æŒçš„ prompt-tempolateï¼Œå¯ä»¥åˆ° XTuner æºç ä¸­çš„ `xtuner/utils/templates.py`â€‹ è¿™ä¸ªæ–‡ä»¶ä¸­è¿›è¡ŒæŸ¥æ‰¾ã€‚

```shell
# ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯
xtuner chat /root/ft/final_model --prompt-template internlm2_chat
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€äº›ç®€å•çš„æµ‹è¯•æ¥çœ‹çœ‹å¾®è°ƒåçš„æ¨¡å‹çš„èƒ½åŠ›ã€‚

> å‡å¦‚æˆ‘ä»¬æƒ³è¦è¾“å…¥å†…å®¹éœ€è¦åœ¨è¾“å…¥æ–‡å­—åæ•²å‡»ä¸¤ä¸‹å›è½¦ï¼Œå‡å¦‚æˆ‘ä»¬æƒ³æ¸…æ¥šå†å²è®°å½•éœ€è¦è¾“å…¥ RESETï¼Œå‡å¦‚æˆ‘ä»¬æƒ³è¦é€€å‡ºåˆ™éœ€è¦è¾“å…¥ EXITã€‚

â€‹![image](assets/image-20240419193831-16m70ye.png)â€‹

å¯ä»¥çœ‹åˆ°æ¨¡å‹å·²ç»ä¸¥é‡è¿‡æ‹Ÿåˆï¼Œå›å¤çš„è¯å°±åªæœ‰ â€œæˆ‘æ˜¯å‰‘é”‹å¤§ä½¬çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦â€ è¿™å¥è¯ã€‚æˆ‘ä»¬ä¸‹é¢å¯ä»¥é€šè¿‡å¯¹æ¯”åŸæ¨¡å‹çš„èƒ½åŠ›æ¥çœ‹çœ‹å·®å¼‚ã€‚

```shell
# åŒæ ·çš„æˆ‘ä»¬ä¹Ÿå¯ä»¥å’ŒåŸæ¨¡å‹è¿›è¡Œå¯¹è¯è¿›è¡Œå¯¹æ¯”
xtuner chat /root/ft/model --prompt-template internlm2_chat
```

æˆ‘ä»¬å¯ä»¥ç”¨åŒæ ·çš„é—®é¢˜æ¥æŸ¥çœ‹å›å¤çš„æƒ…å†µã€‚

```
double enter to end input (EXIT: exit chat, RESET: reset history) >>> ä½ æ˜¯è°
æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å›ç­”é—®é¢˜ã€æä¾›å®šä¹‰å’Œè§£é‡Šã€å°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ã€æ€»ç»“æ–‡æœ¬ã€ç”Ÿæˆæ–‡æœ¬ã€ç¼–å†™æ•…äº‹ã€åˆ†ææƒ…æ„Ÿã€æä¾›æ¨èã€å¼€å‘ç®—æ³•ã€ç¼–å†™ä»£ç ä»¥åŠå…¶ä»–ä»»ä½•åŸºäºè¯­è¨€çš„ä»»åŠ¡ã€‚æˆ‘è‡´åŠ›äºé€šè¿‡æ‰§è¡Œå¸¸è§çš„åŸºäºè¯­è¨€çš„ä»»åŠ¡å’Œæä¾›å»ºè®®æ¥å¸®åŠ©äººç±»ã€‚<|im_end|>

double enter to end input (EXIT: exit chat, RESET: reset history) >>> è¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±
éå¸¸æ„Ÿè°¢æ‚¨çš„æé—®ã€‚æˆ‘æ˜¯ä¸€ä¸ªåå«ä¹¦ç”ŸÂ·æµ¦è¯­çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤å¼€å‘ã€‚æˆ‘ä½¿ç”¨äº†Transformeræ¨¡å‹å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨è¯­è¨€æ¨¡å‹ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ã€‚æˆ‘è‡´åŠ›äºé€šè¿‡æ‰§è¡Œå¸¸è§çš„åŸºäºè¯­è¨€çš„ä»»åŠ¡å’Œæä¾›å»ºè®®æ¥å¸®åŠ©äººç±»ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€æä¾›å®šä¹‰å’Œè§£é‡Šã€å°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ã€æ€»ç»“æ–‡æœ¬ã€ç”Ÿæˆæ–‡æœ¬ã€ç¼–å†™æ•…äº‹ã€åˆ†ææƒ…æ„Ÿã€æä¾›æ¨èã€å¼€å‘ç®—æ³•ã€ç¼–å†™ä»£ç ä»¥åŠå…¶ä»–ä»»ä½•åŸºäºè¯­è¨€çš„ä»»åŠ¡ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•éœ€è¦å¸®åŠ©çš„é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶å‘æˆ‘æé—®ã€‚<|im_end|>

double enter to end input (EXIT: exit chat, RESET: reset history) >>> ä½ æ˜¯æˆ‘çš„å°åŠ©æ‰‹å—
æ˜¯çš„ï¼Œæˆ‘éå¸¸ä¹æ„æˆä¸ºæ‚¨çš„åŠ©æ‰‹ã€‚æˆ‘è‡´åŠ›äºé€šè¿‡æ‰§è¡Œå¸¸è§çš„åŸºäºè¯­è¨€çš„ä»»åŠ¡å’Œæä¾›å»ºè®®æ¥å¸®åŠ©æ‚¨ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•éœ€è¦å¸®åŠ©çš„é—®é¢˜ï¼Œè¯·éšæ—¶å‘æˆ‘æé—®ã€‚æˆ‘ä¼šå°½åŠ›å›ç­”æ‚¨çš„é—®é¢˜å¹¶æä¾›æœ‰ç”¨çš„å»ºè®®ã€‚<|im_end|>

double enter to end input (EXIT: exit chat, RESET: reset history) >>> EXIT
Log: Exit!
```

å¯ä»¥çœ‹åˆ°åœ¨æ²¡æœ‰è¿›è¡Œæˆ‘ä»¬æ•°æ®çš„å¾®è°ƒå‰ï¼ŒåŸæ¨¡å‹æ˜¯èƒ½å¤Ÿè¾“å‡ºæœ‰é€»è¾‘çš„å›å¤ï¼Œå¹¶ä¸”ä¹Ÿä¸ä¼šè®¤ä¸ºä»–æ˜¯æˆ‘ä»¬ç‰¹æœ‰çš„å°åŠ©æ‰‹ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥å¾ˆæ˜æ˜¾çš„çœ‹å‡ºä¸¤è€…ä¹‹é—´çš„å·®å¼‚æ€§ã€‚

é‚£å¯¹äº `xtuner chat`â€‹ è¿™ä¸ªæŒ‡ä»¤è€Œè¨€ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–çš„å‚æ•°å¯ä»¥è¿›è¡Œè®¾ç½®çš„ï¼ŒåŒ…æ‹¬ï¼š

|å¯åŠ¨å‚æ•°|è§£é‡Š|
| ---------------------| -----------------------------------------------------------------------------|
|--system|æŒ‡å®šSYSTEMæ–‡æœ¬ï¼Œç”¨äºåœ¨å¯¹è¯ä¸­æ’å…¥ç‰¹å®šçš„ç³»ç»Ÿçº§ä¿¡æ¯|
|--system-template|æŒ‡å®šSYSTEMæ¨¡æ¿ï¼Œç”¨äºè‡ªå®šä¹‰ç³»ç»Ÿä¿¡æ¯çš„æ¨¡æ¿|
| **--bits**|æŒ‡å®šLLMè¿è¡Œæ—¶ä½¿ç”¨çš„ä½æ•°ï¼Œå†³å®šäº†å¤„ç†æ•°æ®æ—¶çš„ç²¾åº¦|
|--bot-name|è®¾ç½®botçš„åç§°ï¼Œç”¨äºåœ¨å¯¹è¯æˆ–å…¶ä»–äº¤äº’ä¸­è¯†åˆ«bot|
|--with-plugins|æŒ‡å®šåœ¨è¿è¡Œæ—¶è¦ä½¿ç”¨çš„æ’ä»¶åˆ—è¡¨ï¼Œç”¨äºæ‰©å±•æˆ–å¢å¼ºåŠŸèƒ½|
| **--no-streamer**|å…³é—­æµå¼ä¼ è¾“æ¨¡å¼ï¼Œå¯¹äºéœ€è¦ä¸€æ¬¡æ€§å¤„ç†å…¨éƒ¨æ•°æ®çš„åœºæ™¯|
| **--lagent**|å¯ç”¨lagentï¼Œç”¨äºç‰¹å®šçš„è¿è¡Œæ—¶ç¯å¢ƒæˆ–ä¼˜åŒ–|
|--command-stop-word|è®¾ç½®å‘½ä»¤çš„åœæ­¢è¯ï¼Œå½“é‡åˆ°è¿™äº›è¯æ—¶åœæ­¢è§£æå‘½ä»¤|
|--answer-stop-word|è®¾ç½®å›ç­”çš„åœæ­¢è¯ï¼Œå½“ç”Ÿæˆå›ç­”æ—¶é‡åˆ°è¿™äº›è¯åˆ™åœæ­¢|
|--offload-folder|æŒ‡å®šå­˜æ”¾æ¨¡å‹æƒé‡çš„æ–‡ä»¶å¤¹ï¼Œç”¨äºåŠ è½½æˆ–å¸è½½æ¨¡å‹æƒé‡|
|--max-new-tokens|è®¾ç½®ç”Ÿæˆæ–‡æœ¬æ—¶å…è®¸çš„æœ€å¤§tokenæ•°é‡ï¼Œæ§åˆ¶è¾“å‡ºé•¿åº¦|
| **--temperature**|è®¾ç½®ç”Ÿæˆæ–‡æœ¬çš„æ¸©åº¦å€¼ï¼Œè¾ƒé«˜çš„å€¼ä¼šä½¿ç”Ÿæˆçš„æ–‡æœ¬æ›´å¤šæ ·ï¼Œè¾ƒä½çš„å€¼ä¼šä½¿æ–‡æœ¬æ›´ç¡®å®š|
|--top-k|è®¾ç½®ä¿ç•™ç”¨äºé¡¶kç­›é€‰çš„æœ€é«˜æ¦‚ç‡è¯æ±‡æ ‡è®°æ•°ï¼Œå½±å“ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§|
|--top-p|è®¾ç½®ç´¯è®¡æ¦‚ç‡é˜ˆå€¼ï¼Œä»…ä¿ç•™æ¦‚ç‡ç´¯åŠ é«˜äºtop-pçš„æœ€å°æ ‡è®°é›†ï¼Œå½±å“ç”Ÿæˆæ–‡æœ¬çš„è¿è´¯æ€§|
|--seed|è®¾ç½®éšæœºç§å­ï¼Œç”¨äºç”Ÿæˆå¯é‡ç°çš„æ–‡æœ¬å†…å®¹|

é™¤äº†è¿™äº›å‚æ•°ä»¥å¤–å…¶å®è¿˜æœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°å°±æ˜¯ `--adapter`â€‹ ï¼Œè¿™ä¸ªå‚æ•°ä¸»è¦çš„ä½œç”¨å°±æ˜¯å¯ä»¥åœ¨è½¬åŒ–åçš„ adapter å±‚ä¸åŸæ¨¡å‹æ•´åˆä¹‹å‰æ¥å¯¹è¯¥å±‚è¿›è¡Œæµ‹è¯•ã€‚ä½¿ç”¨è¿™ä¸ªé¢å¤–çš„å‚æ•°å¯¹è¯çš„æ¨¡å‹å’Œæ•´åˆåçš„æ¨¡å‹å‡ ä¹æ²¡æœ‰ä»€ä¹ˆå¤ªå¤šçš„åŒºåˆ«ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡æµ‹è¯•ä¸åŒçš„æƒé‡æ–‡ä»¶ç”Ÿæˆçš„ adapter æ¥æ‰¾åˆ°æœ€ä¼˜çš„ adapter è¿›è¡Œæœ€ç»ˆçš„æ¨¡å‹æ•´åˆå·¥ä½œã€‚

```shell
# ä½¿ç”¨ --adapter å‚æ•°ä¸å®Œæ•´çš„æ¨¡å‹è¿›è¡Œå¯¹è¯
xtuner chat /root/ft/model --adapter /root/ft/huggingface --prompt-template internlm2_chat
```

â€

> ç”±äºä¸çŸ¥é“ä»€ä¹ˆåŸå› ä¸€ç›´æ— æ³•è¿æ¥ä¸ŠæœåŠ¡å™¨ï¼Œæ‰€ä»¥å°±ä¸è¿›è¡ŒWebéƒ¨ç½²é¢

â€

#### 2.5.4 Web demo éƒ¨ç½²

é™¤äº†åœ¨ç»ˆç«¯ä¸­å¯¹æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬å…¶å®è¿˜å¯ä»¥åœ¨ç½‘é¡µç«¯çš„ demo è¿›è¡Œå¯¹è¯ã€‚

é‚£é¦–å…ˆæˆ‘ä»¬éœ€è¦å…ˆä¸‹è½½ç½‘é¡µç«¯ web demo æ‰€éœ€è¦çš„ä¾èµ–ã€‚

```shell
pip install streamlit==1.24.0
```

ä¸‹è½½ [InternLM](https://github.com/InternLM/InternLM) é¡¹ç›®ä»£ç ï¼ˆæ¬¢è¿Starï¼‰ï¼

```shell
# åˆ›å»ºå­˜æ”¾ InternLM æ–‡ä»¶çš„ä»£ç 
mkdir -p /root/ft/web_demo && cd /root/ft/web_demo

# æ‹‰å– InternLM æºæ–‡ä»¶
git clone https://github.com/InternLM/InternLM.git

# è¿›å…¥è¯¥åº“ä¸­
cd /root/ft/web_demo/InternLM
```

å°† `/root/ft/web_demo/InternLM/chat/web_demo.py`â€‹ ä¸­çš„å†…å®¹æ›¿æ¢ä¸ºä»¥ä¸‹çš„ä»£ç ï¼ˆä¸æºä»£ç ç›¸æ¯”ï¼Œæ­¤å¤„ä¿®æ”¹äº†æ¨¡å‹è·¯å¾„å’Œåˆ†è¯å™¨è·¯å¾„ï¼Œå¹¶ä¸”ä¹Ÿåˆ é™¤äº† avatar åŠ system_prompt éƒ¨åˆ†çš„å†…å®¹ï¼ŒåŒæ—¶ä¸ cli ä¸­çš„è¶…å‚æ•°è¿›è¡Œäº†å¯¹é½ï¼‰ã€‚

```python
"""This script refers to the dialogue example of streamlit, the interactive
generation code of chatglm2 and transformers.

We mainly modified part of the code logic to adapt to the
generation of our model.
Please refer to these links below for more information:
    1. streamlit chat example:
        https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps
    2. chatglm2:
        https://github.com/THUDM/ChatGLM2-6B
    3. transformers:
        https://github.com/huggingface/transformers
Please run with the command `streamlit run path/to/web_demo.py
    --server.address=0.0.0.0 --server.port 7860`.
Using `python path/to/web_demo.py` may cause unknown problems.
"""
# isort: skip_file
import copy
import warnings
from dataclasses import asdict, dataclass
from typing import Callable, List, Optional

import streamlit as st
import torch
from torch import nn
from transformers.generation.utils import (LogitsProcessorList,
                                           StoppingCriteriaList)
from transformers.utils import logging

from transformers import AutoTokenizer, AutoModelForCausalLM  # isort: skip

logger = logging.get_logger(__name__)


@dataclass
class GenerationConfig:
    # this config is used for chat to provide more diversity
    max_length: int = 2048
    top_p: float = 0.75
    temperature: float = 0.1
    do_sample: bool = True
    repetition_penalty: float = 1.000


@torch.inference_mode()
def generate_interactive(
    model,
    tokenizer,
    prompt,
    generation_config: Optional[GenerationConfig] = None,
    logits_processor: Optional[LogitsProcessorList] = None,
    stopping_criteria: Optional[StoppingCriteriaList] = None,
    prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor],
                                                List[int]]] = None,
    additional_eos_token_id: Optional[int] = None,
    **kwargs,
):
    inputs = tokenizer([prompt], padding=True, return_tensors='pt')
    input_length = len(inputs['input_ids'][0])
    for k, v in inputs.items():
        inputs[k] = v.cuda()
    input_ids = inputs['input_ids']
    _, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]
    if generation_config is None:
        generation_config = model.generation_config
    generation_config = copy.deepcopy(generation_config)
    model_kwargs = generation_config.update(**kwargs)
    bos_token_id, eos_token_id = (  # noqa: F841  # pylint: disable=W0612
        generation_config.bos_token_id,
        generation_config.eos_token_id,
    )
    if isinstance(eos_token_id, int):
        eos_token_id = [eos_token_id]
    if additional_eos_token_id is not None:
        eos_token_id.append(additional_eos_token_id)
    has_default_max_length = kwargs.get(
        'max_length') is None and generation_config.max_length is not None
    if has_default_max_length and generation_config.max_new_tokens is None:
        warnings.warn(
            f"Using 'max_length''s default ({repr(generation_config.max_length)}) \
                to control the generation length. "
            'This behaviour is deprecated and will be removed from the \
                config in v5 of Transformers -- we'
            ' recommend using `max_new_tokens` to control the maximum \
                length of the generation.',
            UserWarning,
        )
    elif generation_config.max_new_tokens is not None:
        generation_config.max_length = generation_config.max_new_tokens + \
            input_ids_seq_length
        if not has_default_max_length:
            logger.warn(  # pylint: disable=W4902
                f"Both 'max_new_tokens' (={generation_config.max_new_tokens}) "
                f"and 'max_length'(={generation_config.max_length}) seem to "
                "have been set. 'max_new_tokens' will take precedence. "
                'Please refer to the documentation for more information. '
                '(https://huggingface.co/docs/transformers/main/'
                'en/main_classes/text_generation)',
                UserWarning,
            )

    if input_ids_seq_length >= generation_config.max_length:
        input_ids_string = 'input_ids'
        logger.warning(
            f"Input length of {input_ids_string} is {input_ids_seq_length}, "
            f"but 'max_length' is set to {generation_config.max_length}. "
            'This can lead to unexpected behavior. You should consider'
            " increasing 'max_new_tokens'.")

    # 2. Set generation parameters if not already defined
    logits_processor = logits_processor if logits_processor is not None \
        else LogitsProcessorList()
    stopping_criteria = stopping_criteria if stopping_criteria is not None \
        else StoppingCriteriaList()

    logits_processor = model._get_logits_processor(
        generation_config=generation_config,
        input_ids_seq_length=input_ids_seq_length,
        encoder_input_ids=input_ids,
        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
        logits_processor=logits_processor,
    )

    stopping_criteria = model._get_stopping_criteria(
        generation_config=generation_config,
        stopping_criteria=stopping_criteria)
    logits_warper = model._get_logits_warper(generation_config)

    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)
    scores = None
    while True:
        model_inputs = model.prepare_inputs_for_generation(
            input_ids, **model_kwargs)
        # forward pass to get next token
        outputs = model(
            **model_inputs,
            return_dict=True,
            output_attentions=False,
            output_hidden_states=False,
        )

        next_token_logits = outputs.logits[:, -1, :]

        # pre-process distribution
        next_token_scores = logits_processor(input_ids, next_token_logits)
        next_token_scores = logits_warper(input_ids, next_token_scores)

        # sample
        probs = nn.functional.softmax(next_token_scores, dim=-1)
        if generation_config.do_sample:
            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
        else:
            next_tokens = torch.argmax(probs, dim=-1)

        # update generated ids, model inputs, and length for next step
        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
        model_kwargs = model._update_model_kwargs_for_generation(
            outputs, model_kwargs, is_encoder_decoder=False)
        unfinished_sequences = unfinished_sequences.mul(
            (min(next_tokens != i for i in eos_token_id)).long())

        output_token_ids = input_ids[0].cpu().tolist()
        output_token_ids = output_token_ids[input_length:]
        for each_eos_token_id in eos_token_id:
            if output_token_ids[-1] == each_eos_token_id:
                output_token_ids = output_token_ids[:-1]
        response = tokenizer.decode(output_token_ids)

        yield response
        # stop when each sentence is finished
        # or if we exceed the maximum length
        if unfinished_sequences.max() == 0 or stopping_criteria(
                input_ids, scores):
            break


def on_btn_click():
    del st.session_state.messages


@st.cache_resource
def load_model():
    model = (AutoModelForCausalLM.from_pretrained('/root/ft/final_model',
                                                  trust_remote_code=True).to(
                                                      torch.bfloat16).cuda())
    tokenizer = AutoTokenizer.from_pretrained('/root/ft/final_model',
                                              trust_remote_code=True)
    return model, tokenizer


def prepare_generation_config():
    with st.sidebar:
        max_length = st.slider('Max Length',
                               min_value=8,
                               max_value=32768,
                               value=2048)
        top_p = st.slider('Top P', 0.0, 1.0, 0.75, step=0.01)
        temperature = st.slider('Temperature', 0.0, 1.0, 0.1, step=0.01)
        st.button('Clear Chat History', on_click=on_btn_click)

    generation_config = GenerationConfig(max_length=max_length,
                                         top_p=top_p,
                                         temperature=temperature)

    return generation_config


user_prompt = '<|im_start|>user\n{user}<|im_end|>\n'
robot_prompt = '<|im_start|>assistant\n{robot}<|im_end|>\n'
cur_query_prompt = '<|im_start|>user\n{user}<|im_end|>\n\
    <|im_start|>assistant\n'


def combine_history(prompt):
    messages = st.session_state.messages
    meta_instruction = ('')
    total_prompt = f"<s><|im_start|>system\n{meta_instruction}<|im_end|>\n"
    for message in messages:
        cur_content = message['content']
        if message['role'] == 'user':
            cur_prompt = user_prompt.format(user=cur_content)
        elif message['role'] == 'robot':
            cur_prompt = robot_prompt.format(robot=cur_content)
        else:
            raise RuntimeError
        total_prompt += cur_prompt
    total_prompt = total_prompt + cur_query_prompt.format(user=prompt)
    return total_prompt


def main():
    # torch.cuda.empty_cache()
    print('load model begin.')
    model, tokenizer = load_model()
    print('load model end.')


    st.title('InternLM2-Chat-1.8B')

    generation_config = prepare_generation_config()

    # Initialize chat history
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message['role'], avatar=message.get('avatar')):
            st.markdown(message['content'])

    # Accept user input
    if prompt := st.chat_input('What is up?'):
        # Display user message in chat message container
        with st.chat_message('user'):
            st.markdown(prompt)
        real_prompt = combine_history(prompt)
        # Add user message to chat history
        st.session_state.messages.append({
            'role': 'user',
            'content': prompt,
        })

        with st.chat_message('robot'):
            message_placeholder = st.empty()
            for cur_response in generate_interactive(
                    model=model,
                    tokenizer=tokenizer,
                    prompt=real_prompt,
                    additional_eos_token_id=92542,
                    **asdict(generation_config),
            ):
                # Display robot response in chat message container
                message_placeholder.markdown(cur_response + 'â–Œ')
            message_placeholder.markdown(cur_response)
        # Add robot response to chat history
        st.session_state.messages.append({
            'role': 'robot',
            'content': cur_response,  # pylint: disable=undefined-loop-variable
        })
        torch.cuda.empty_cache()


if __name__ == '__main__':
    main()
```

åœ¨è¿è¡Œå‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åšçš„å°±æ˜¯å°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚é‚£é¦–å…ˆæˆ‘ä»¬ä½¿ç”¨å¿«æ·é”®ç»„åˆ `Windows + R`â€‹â€‹ï¼ˆWindows å³å¼€å§‹èœå•é”®ï¼‰æ‰“å¼€æŒ‡ä»¤ç•Œé¢ï¼Œå¹¶è¾“å…¥å‘½ä»¤ï¼ŒæŒ‰ä¸‹å›è½¦é”®ã€‚ï¼ˆMac ç”¨æˆ·æ‰“å¼€ç»ˆç«¯å³å¯ï¼‰

æ‰“å¼€ PowerShell åï¼Œå…ˆæŸ¥è¯¢ç«¯å£ï¼Œå†æ ¹æ®ç«¯å£é”®å…¥å‘½ä»¤ ï¼ˆä¾‹å¦‚å›¾ä¸­ç«¯å£ç¤ºä¾‹ä¸º 38374ï¼‰ï¼š

â€

ç„¶åæˆ‘ä»¬éœ€è¦åœ¨ PowerShell ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼ˆéœ€è¦æ›¿æ¢ä¸ºè‡ªå·±çš„ç«¯å£å·ï¼‰

```shell
# ä»æœ¬åœ°ä½¿ç”¨ ssh è¿æ¥ studio ç«¯å£
# å°†ä¸‹æ–¹ç«¯å£å· 38374 æ›¿æ¢æˆè‡ªå·±çš„ç«¯å£å·
ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p 38374
```

å†å¤åˆ¶ä¸‹æ–¹çš„å¯†ç ï¼Œè¾“å…¥åˆ° `password`â€‹â€‹ ä¸­ï¼Œç›´æ¥å›è½¦ï¼š

æœ€ç»ˆä¿æŒåœ¨å¦‚ä¸‹æ•ˆæœå³å¯ï¼š

ä¹‹åæˆ‘ä»¬éœ€è¦è¾“å…¥ä»¥ä¸‹å‘½ä»¤è¿è¡Œ `/root/personal_assistant/code/InternLM`â€‹ ç›®å½•ä¸‹çš„ `web_demo.py`â€‹ æ–‡ä»¶ã€‚

```shell
streamlit run /root/ft/web_demo/InternLM/chat/web_demo.py --server.address 127.0.0.1 --server.port 6006
```

> æ³¨æ„ï¼šè¦åœ¨æµè§ˆå™¨æ‰“å¼€ `http://127.0.0.1:6006`â€‹ é¡µé¢åï¼Œæ¨¡å‹æ‰ä¼šåŠ è½½ã€‚

æ‰“å¼€ [http://127.0.0.1:6006](http://127.0.0.1:6006/) åï¼Œç­‰å¾…åŠ è½½å®Œæˆå³å¯è¿›è¡Œå¯¹è¯ï¼Œé”®å…¥å†…å®¹ç¤ºä¾‹å¦‚ä¸‹ï¼š

```
è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±
```

æ•ˆæœå›¾å¦‚ä¸‹ï¼š

å‡å¦‚æˆ‘ä»¬è¿˜æƒ³å’ŒåŸæ¥çš„ InternLM2-Chat-1.8B æ¨¡å‹å¯¹è¯ï¼ˆå³åœ¨ `/root/ft/model`â€‹ è¿™é‡Œçš„æ¨¡å‹å¯¹è¯ï¼‰ï¼Œæˆ‘ä»¬å…¶å®åªéœ€è¦ä¿®æ”¹183è¡Œå’Œ186è¡Œçš„æ–‡ä»¶åœ°å€å³å¯ã€‚

```diff
# ä¿®æ”¹æ¨¡å‹åœ°å€ï¼ˆç¬¬183è¡Œï¼‰
- model = (AutoModelForCausalLM.from_pretrained('/root/ft/final_model',
+ model = (AutoModelForCausalLM.from_pretrained('/root/ft/model',

# ä¿®æ”¹åˆ†è¯å™¨åœ°å€ï¼ˆç¬¬186è¡Œï¼‰
- tokenizer = AutoTokenizer.from_pretrained('/root/ft/final_model',
+ tokenizer = AutoTokenizer.from_pretrained('/root/ft/model',
```

ç„¶åä½¿ç”¨ä¸Šæ–¹åŒæ ·çš„å‘½ä»¤å³å¯è¿è¡Œã€‚

```shell
streamlit run /root/ft/web_demo/InternLM/chat/web_demo.py --server.address 127.0.0.1 --server.port 6006
```

åŠ è½½å®Œæˆåè¾“å…¥åŒæ ·çš„é—®é¢˜ `è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±`â€‹ ä¹‹åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªæ¨¡å‹ç„¶ä¸åŒçš„å›å¤ï¼š

#### 2.5.5 å°ç»“

åœ¨è¿™ä¸€å°èŠ‚é‡Œæˆ‘ä»¬å¯¹å¾®è°ƒåçš„æ¨¡å‹ï¼ˆadapterï¼‰è¿›è¡Œäº†è½¬æ¢åŠæ•´åˆçš„æ“ä½œï¼Œå¹¶é€šè¿‡ `xtuner chat`â€‹ æ¥å¯¹æ¨¡å‹è¿›è¡Œäº†å®é™…çš„å¯¹è¯æµ‹è¯•ã€‚ä»ç»“æœå¯ä»¥æ¸…æ¥šçš„çœ‹å‡ºæ¨¡å‹çš„å›å¤åœ¨å¾®è°ƒçš„å‰åå‡ºç°äº†æ˜æ˜¾çš„å˜åŒ–ã€‚é‚£å½“æˆ‘ä»¬åœ¨æµ‹è¯•å®Œæ¨¡å‹è®¤ä¸ºå…¶æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚åï¼Œæˆ‘ä»¬å°±å¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–éƒ¨ç½²ç­‰æ“ä½œäº†ï¼Œè¿™éƒ¨åˆ†çš„å†…å®¹åœ¨ä¹‹åå…³äº LMDeploy çš„è¯¾ç¨‹ä¸­å°†ä¼šè¯¦ç»†çš„è¿›è¡Œè®²è§£ï¼Œæ•¬è¯·æœŸå¾…åç»­çš„è¯¾ç¨‹å§ï¼

### 2.6 æ€»ç»“

åœ¨æœ¬èŠ‚ä¸­ä¸»è¦å°±æ˜¯å¸¦é¢†ç€å¤§å®¶è·‘é€šäº† XTuner çš„ä¸€ä¸ªå®Œæ•´æµç¨‹ï¼Œé€šè¿‡äº†è§£æ•°æ®é›†å’Œæ¨¡å‹çš„ä½¿ç”¨æ–¹æ³•ã€é…ç½®æ–‡ä»¶çš„åˆ¶ä½œå’Œè®­ç»ƒä»¥åŠæœ€åçš„è½¬æ¢åŠæ•´åˆã€‚é‚£åœ¨åé¢å‡å¦‚æˆ‘ä»¬ä¹Ÿæœ‰æƒ³è¦å¾®è°ƒå‡ºè‡ªå·±çš„ä¸€ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨åŒæ ·æµç¨‹å’Œæ–¹æ³•è¿›è¡Œè¿›ä¸€æ­¥çš„å®è·µï¼

â€

â€

# å¤šæ¨¡æ€LLMåŸç†

â€

å•æ¨¡æ€å’Œå¤šæ¨¡æ€çš„åŒºåˆ«åœ¨äºè¾“å…¥æ—¶çš„ä¿¡æ¯æµæ•°é‡ï¼Œå›¾åƒå¤šæ¨¡æ€è®­ç»ƒçš„å°±æ˜¯Image Projectorï¼Œå¤šæ¨¡æ€å¹¶ä¸æŒ‡ç”Ÿå›¾ï¼Œè€Œæ˜¯è¯†åˆ«å›¾ç‰‡ï¼Œç†è§£å›¾ç‰‡ã€‚

â€

â€‹![image](assets/image-20240419005254-xv8t0st.png)â€‹

â€

### LLMåŠ è£…è§†è§‰æ–¹æ¡ˆ

Haotian Liuç­‰ä½¿ç”¨GPT-4Vå¯¹å›¾åƒæ•°æ®ç”Ÿæˆæè¿°ï¼Œä»¥æ­¤æ„å»ºå‡ºå¤§é‡  
<question text><image>-<answer text>çš„æ•°æ®å¯¹ã€‚  
åˆ©ç”¨è¿™äº›æ•°æ®å¯¹ï¼Œé…åˆæ–‡æœ¬å•æ¨¡æ€LLM,è®­ç»ƒå‡ºä¸€ä¸ªImage Projectorã€‚  
æ‰€ä½¿ç”¨çš„æ–‡æœ¬å•æ¨¡å‹LLMå’Œè®­ç»ƒå‡ºæ¥çš„Image Projector,ç»Ÿç§°ä¸ºLLaVAæ¨¡å‹ã€‚

â€

â€‹![image](assets/image-20240419005606-ek9qd6j.png)â€‹

â€

## å¤šæ¨¡æ€è®­ç»ƒå¿«é€Ÿä¸Šæ‰‹

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è‡ªå·±æ„é€ <questiontext><image>--<answer text>æ•°æ®å¯¹ï¼ŒåŸºäºInternLM2Chat1.8Bè¿™ä¸ªæ–‡æœ¬å•æ¨¡æ€æ¨¡å‹ï¼Œä½¿ç”¨LLaVAæ–¹æ¡ˆï¼Œè®­ç»ƒä¸€ä¸ªç»™InternLM.2Chat1.8Bä½¿ç”¨çš„Image Projectoræ–‡ä»¶ã€‚

â€‹![image](assets/image-20240419005805-ultwj4w.png)â€‹

â€
